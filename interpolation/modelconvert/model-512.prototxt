name: "pytorch"
input: "data1"
input: "data2"
input_shape {
  dim: 1
  dim: 3
  dim: 512
  dim: 512
}
input_shape {
  dim: 1
  dim: 3
  dim: 512
  dim: 512
}
layer {
    name: "ConvNdBackward1"
    type: "Convolution"
    bottom: "data1"
    top: "ConvNdBackward1"
    convolution_param {
        num_output: 16
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 2
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward2"
    type: "ReLU"
    bottom: "ConvNdBackward1"
    top: "LeakyReluBackward2"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward3"
    type: "Convolution"
    bottom: "LeakyReluBackward2"
    top: "ConvNdBackward3"
    convolution_param {
        num_output: 16
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward4"
    type: "ReLU"
    bottom: "ConvNdBackward3"
    top: "LeakyReluBackward4"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward5"
    type: "Convolution"
    bottom: "data2"
    top: "ConvNdBackward5"
    convolution_param {
        num_output: 16
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 2
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward6"
    type: "ReLU"
    bottom: "ConvNdBackward5"
    top: "LeakyReluBackward6"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward7"
    type: "Convolution"
    bottom: "LeakyReluBackward6"
    top: "ConvNdBackward7"
    convolution_param {
        num_output: 16
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward8"
    type: "ReLU"
    bottom: "ConvNdBackward7"
    top: "LeakyReluBackward8"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward10"
    type: "Convolution"
    bottom: "LeakyReluBackward4"
    top: "ConvNdBackward10"
    convolution_param {
        num_output: 32
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 2
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward11"
    type: "ReLU"
    bottom: "ConvNdBackward10"
    top: "LeakyReluBackward11"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward12"
    type: "Convolution"
    bottom: "LeakyReluBackward11"
    top: "ConvNdBackward12"
    convolution_param {
        num_output: 32
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward13"
    type: "ReLU"
    bottom: "ConvNdBackward12"
    top: "LeakyReluBackward13"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward15"
    type: "Convolution"
    bottom: "LeakyReluBackward8"
    top: "ConvNdBackward15"
    convolution_param {
        num_output: 32
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 2
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward16"
    type: "ReLU"
    bottom: "ConvNdBackward15"
    top: "LeakyReluBackward16"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward17"
    type: "Convolution"
    bottom: "LeakyReluBackward16"
    top: "ConvNdBackward17"
    convolution_param {
        num_output: 32
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward18"
    type: "ReLU"
    bottom: "ConvNdBackward17"
    top: "LeakyReluBackward18"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward20"
    type: "Convolution"
    bottom: "LeakyReluBackward13"
    top: "ConvNdBackward20"
    convolution_param {
        num_output: 64
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 2
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward21"
    type: "ReLU"
    bottom: "ConvNdBackward20"
    top: "LeakyReluBackward21"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward22"
    type: "Convolution"
    bottom: "LeakyReluBackward21"
    top: "ConvNdBackward22"
    convolution_param {
        num_output: 64
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward23"
    type: "ReLU"
    bottom: "ConvNdBackward22"
    top: "LeakyReluBackward23"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward25"
    type: "Convolution"
    bottom: "LeakyReluBackward18"
    top: "ConvNdBackward25"
    convolution_param {
        num_output: 64
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 2
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward26"
    type: "ReLU"
    bottom: "ConvNdBackward25"
    top: "LeakyReluBackward26"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward27"
    type: "Convolution"
    bottom: "LeakyReluBackward26"
    top: "ConvNdBackward27"
    convolution_param {
        num_output: 64
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward28"
    type: "ReLU"
    bottom: "ConvNdBackward27"
    top: "LeakyReluBackward28"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward30"
    type: "Convolution"
    bottom: "LeakyReluBackward23"
    top: "ConvNdBackward30"
    convolution_param {
        num_output: 96
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 2
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward31"
    type: "ReLU"
    bottom: "ConvNdBackward30"
    top: "LeakyReluBackward31"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward32"
    type: "Convolution"
    bottom: "LeakyReluBackward31"
    top: "ConvNdBackward32"
    convolution_param {
        num_output: 96
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward33"
    type: "ReLU"
    bottom: "ConvNdBackward32"
    top: "LeakyReluBackward33"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward35"
    type: "Convolution"
    bottom: "LeakyReluBackward28"
    top: "ConvNdBackward35"
    convolution_param {
        num_output: 96
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 2
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward36"
    type: "ReLU"
    bottom: "ConvNdBackward35"
    top: "LeakyReluBackward36"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward37"
    type: "Convolution"
    bottom: "LeakyReluBackward36"
    top: "ConvNdBackward37"
    convolution_param {
        num_output: 96
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward38"
    type: "ReLU"
    bottom: "ConvNdBackward37"
    top: "LeakyReluBackward38"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward40"
    type: "Convolution"
    bottom: "LeakyReluBackward33"
    top: "ConvNdBackward40"
    convolution_param {
        num_output: 128
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 2
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward41"
    type: "ReLU"
    bottom: "ConvNdBackward40"
    top: "LeakyReluBackward41"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward42"
    type: "Convolution"
    bottom: "LeakyReluBackward41"
    top: "ConvNdBackward42"
    convolution_param {
        num_output: 128
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward43"
    type: "ReLU"
    bottom: "ConvNdBackward42"
    top: "LeakyReluBackward43"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward45"
    type: "Convolution"
    bottom: "LeakyReluBackward38"
    top: "ConvNdBackward45"
    convolution_param {
        num_output: 128
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 2
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward46"
    type: "ReLU"
    bottom: "ConvNdBackward45"
    top: "LeakyReluBackward46"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward47"
    type: "Convolution"
    bottom: "LeakyReluBackward46"
    top: "ConvNdBackward47"
    convolution_param {
        num_output: 128
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward48"
    type: "ReLU"
    bottom: "ConvNdBackward47"
    top: "LeakyReluBackward48"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward50"
    type: "Convolution"
    bottom: "LeakyReluBackward43"
    top: "ConvNdBackward50"
    convolution_param {
        num_output: 192
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 2
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward51"
    type: "ReLU"
    bottom: "ConvNdBackward50"
    top: "LeakyReluBackward51"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward52"
    type: "Convolution"
    bottom: "LeakyReluBackward51"
    top: "ConvNdBackward52"
    convolution_param {
        num_output: 192
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward53"
    type: "ReLU"
    bottom: "ConvNdBackward52"
    top: "LeakyReluBackward53"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward55"
    type: "Convolution"
    bottom: "LeakyReluBackward48"
    top: "ConvNdBackward55"
    convolution_param {
        num_output: 192
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 2
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward56"
    type: "ReLU"
    bottom: "ConvNdBackward55"
    top: "LeakyReluBackward56"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward57"
    type: "Convolution"
    bottom: "LeakyReluBackward56"
    top: "ConvNdBackward57"
    convolution_param {
        num_output: 192
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward58"
    type: "ReLU"
    bottom: "ConvNdBackward57"
    top: "LeakyReluBackward58"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "IndexBackward60"
    type: "Slice"
    bottom: "LeakyReluBackward58"
    top: "IndexBackward60"
    top: "IndexBackward60_zcam"
    slice_param {
        axis: 1
        slice_point: 2
    }
}
layer {
    name: "SubBackward162"
    type: "Eltwise"
    bottom: "IndexBackward60"
    bottom: "IndexBackward60"
    top: "SubBackward162"
    eltwise_param {
        operation: SUM
        coeff: 1
        coeff: -1
    }
}
layer {
    name: "Resample2dFunctionBackward63"
    type: "FlowWarp"
    bottom: "LeakyReluBackward58"
    bottom: "SubBackward162"
    top: "Resample2dFunctionBackward63"
}
layer {
    name: "CatBackward65"
    type: "Concat"
    bottom: "LeakyReluBackward53"
    bottom: "Resample2dFunctionBackward63"
    bottom: "SubBackward162"
    top: "CatBackward65"
    concat_param {
        axis: 1
    }
}
layer {
    name: "ConvNdBackward66"
    type: "Convolution"
    bottom: "CatBackward65"
    top: "ConvNdBackward66"
    convolution_param {
        num_output: 128
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward67"
    type: "ReLU"
    bottom: "ConvNdBackward66"
    top: "LeakyReluBackward67"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward68"
    type: "Convolution"
    bottom: "LeakyReluBackward67"
    top: "ConvNdBackward68"
    convolution_param {
        num_output: 128
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward69"
    type: "ReLU"
    bottom: "ConvNdBackward68"
    top: "LeakyReluBackward69"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward70"
    type: "Convolution"
    bottom: "LeakyReluBackward69"
    top: "ConvNdBackward70"
    convolution_param {
        num_output: 96
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward71"
    type: "ReLU"
    bottom: "ConvNdBackward70"
    top: "LeakyReluBackward71"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward72"
    type: "Convolution"
    bottom: "LeakyReluBackward71"
    top: "ConvNdBackward72"
    convolution_param {
        num_output: 64
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward73"
    type: "ReLU"
    bottom: "ConvNdBackward72"
    top: "LeakyReluBackward73"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward74"
    type: "Convolution"
    bottom: "LeakyReluBackward73"
    top: "ConvNdBackward74"
    convolution_param {
        num_output: 32
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward75"
    type: "ReLU"
    bottom: "ConvNdBackward74"
    top: "LeakyReluBackward75"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward76"
    type: "Convolution"
    bottom: "LeakyReluBackward75"
    top: "ConvNdBackward76"
    convolution_param {
        num_output: 2
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "UpsamplingBilinear2dBackward77"
    type: "Deconvolution"
    bottom: "ConvNdBackward76"
    top: "UpsamplingBilinear2dBackward77"
    convolution_param {
        num_output: 2
        group: 2
        kernel_size: 4
        stride: 2
        pad: 1
        weight_filler {
            type: "bilinear"
        }
        bias_term: false
    }
    param {
        lr_mult: 0
        decay_mult: 0
    }
}
layer {
    name: "Resample2dFunctionBackward78"
    type: "FlowWarp"
    bottom: "LeakyReluBackward48"
    bottom: "UpsamplingBilinear2dBackward77"
    top: "Resample2dFunctionBackward78"
}
layer {
    name: "CatBackward80"
    type: "Concat"
    bottom: "LeakyReluBackward43"
    bottom: "Resample2dFunctionBackward78"
    bottom: "UpsamplingBilinear2dBackward77"
    top: "CatBackward80"
    concat_param {
        axis: 1
    }
}
layer {
    name: "ConvNdBackward81"
    type: "Convolution"
    bottom: "CatBackward80"
    top: "ConvNdBackward81"
    convolution_param {
        num_output: 128
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward82"
    type: "ReLU"
    bottom: "ConvNdBackward81"
    top: "LeakyReluBackward82"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward83"
    type: "Convolution"
    bottom: "LeakyReluBackward82"
    top: "ConvNdBackward83"
    convolution_param {
        num_output: 128
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward84"
    type: "ReLU"
    bottom: "ConvNdBackward83"
    top: "LeakyReluBackward84"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward85"
    type: "Convolution"
    bottom: "LeakyReluBackward84"
    top: "ConvNdBackward85"
    convolution_param {
        num_output: 96
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward86"
    type: "ReLU"
    bottom: "ConvNdBackward85"
    top: "LeakyReluBackward86"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward87"
    type: "Convolution"
    bottom: "LeakyReluBackward86"
    top: "ConvNdBackward87"
    convolution_param {
        num_output: 64
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward88"
    type: "ReLU"
    bottom: "ConvNdBackward87"
    top: "LeakyReluBackward88"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward89"
    type: "Convolution"
    bottom: "LeakyReluBackward88"
    top: "ConvNdBackward89"
    convolution_param {
        num_output: 32
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward90"
    type: "ReLU"
    bottom: "ConvNdBackward89"
    top: "LeakyReluBackward90"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward91"
    type: "Convolution"
    bottom: "LeakyReluBackward90"
    top: "ConvNdBackward91"
    convolution_param {
        num_output: 2
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "UpsamplingBilinear2dBackward92"
    type: "Deconvolution"
    bottom: "ConvNdBackward91"
    top: "UpsamplingBilinear2dBackward92"
    convolution_param {
        num_output: 2
        group: 2
        kernel_size: 4
        stride: 2
        pad: 1
        weight_filler {
            type: "bilinear"
        }
        bias_term: false
    }
    param {
        lr_mult: 0
        decay_mult: 0
    }
}
layer {
    name: "Resample2dFunctionBackward93"
    type: "FlowWarp"
    bottom: "LeakyReluBackward38"
    bottom: "UpsamplingBilinear2dBackward92"
    top: "Resample2dFunctionBackward93"
}
layer {
    name: "CatBackward95"
    type: "Concat"
    bottom: "LeakyReluBackward33"
    bottom: "Resample2dFunctionBackward93"
    bottom: "UpsamplingBilinear2dBackward92"
    top: "CatBackward95"
    concat_param {
        axis: 1
    }
}
layer {
    name: "ConvNdBackward96"
    type: "Convolution"
    bottom: "CatBackward95"
    top: "ConvNdBackward96"
    convolution_param {
        num_output: 128
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward97"
    type: "ReLU"
    bottom: "ConvNdBackward96"
    top: "LeakyReluBackward97"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward98"
    type: "Convolution"
    bottom: "LeakyReluBackward97"
    top: "ConvNdBackward98"
    convolution_param {
        num_output: 128
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward99"
    type: "ReLU"
    bottom: "ConvNdBackward98"
    top: "LeakyReluBackward99"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward100"
    type: "Convolution"
    bottom: "LeakyReluBackward99"
    top: "ConvNdBackward100"
    convolution_param {
        num_output: 96
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward101"
    type: "ReLU"
    bottom: "ConvNdBackward100"
    top: "LeakyReluBackward101"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward102"
    type: "Convolution"
    bottom: "LeakyReluBackward101"
    top: "ConvNdBackward102"
    convolution_param {
        num_output: 64
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward103"
    type: "ReLU"
    bottom: "ConvNdBackward102"
    top: "LeakyReluBackward103"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward104"
    type: "Convolution"
    bottom: "LeakyReluBackward103"
    top: "ConvNdBackward104"
    convolution_param {
        num_output: 32
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward105"
    type: "ReLU"
    bottom: "ConvNdBackward104"
    top: "LeakyReluBackward105"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward106"
    type: "Convolution"
    bottom: "LeakyReluBackward105"
    top: "ConvNdBackward106"
    convolution_param {
        num_output: 2
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "UpsamplingBilinear2dBackward107"
    type: "Deconvolution"
    bottom: "ConvNdBackward106"
    top: "UpsamplingBilinear2dBackward107"
    convolution_param {
        num_output: 2
        group: 2
        kernel_size: 4
        stride: 2
        pad: 1
        weight_filler {
            type: "bilinear"
        }
        bias_term: false
    }
    param {
        lr_mult: 0
        decay_mult: 0
    }
}
layer {
    name: "Resample2dFunctionBackward108"
    type: "FlowWarp"
    bottom: "LeakyReluBackward28"
    bottom: "UpsamplingBilinear2dBackward107"
    top: "Resample2dFunctionBackward108"
}
layer {
    name: "CatBackward110"
    type: "Concat"
    bottom: "LeakyReluBackward23"
    bottom: "Resample2dFunctionBackward108"
    bottom: "UpsamplingBilinear2dBackward107"
    top: "CatBackward110"
    concat_param {
        axis: 1
    }
}
layer {
    name: "ConvNdBackward111"
    type: "Convolution"
    bottom: "CatBackward110"
    top: "ConvNdBackward111"
    convolution_param {
        num_output: 128
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward112"
    type: "ReLU"
    bottom: "ConvNdBackward111"
    top: "LeakyReluBackward112"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward113"
    type: "Convolution"
    bottom: "LeakyReluBackward112"
    top: "ConvNdBackward113"
    convolution_param {
        num_output: 128
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward114"
    type: "ReLU"
    bottom: "ConvNdBackward113"
    top: "LeakyReluBackward114"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward115"
    type: "Convolution"
    bottom: "LeakyReluBackward114"
    top: "ConvNdBackward115"
    convolution_param {
        num_output: 96
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward116"
    type: "ReLU"
    bottom: "ConvNdBackward115"
    top: "LeakyReluBackward116"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward117"
    type: "Convolution"
    bottom: "LeakyReluBackward116"
    top: "ConvNdBackward117"
    convolution_param {
        num_output: 64
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward118"
    type: "ReLU"
    bottom: "ConvNdBackward117"
    top: "LeakyReluBackward118"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward119"
    type: "Convolution"
    bottom: "LeakyReluBackward118"
    top: "ConvNdBackward119"
    convolution_param {
        num_output: 32
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward120"
    type: "ReLU"
    bottom: "ConvNdBackward119"
    top: "LeakyReluBackward120"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward121"
    type: "Convolution"
    bottom: "LeakyReluBackward120"
    top: "ConvNdBackward121"
    convolution_param {
        num_output: 2
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "UpsamplingBilinear2dBackward122"
    type: "Deconvolution"
    bottom: "ConvNdBackward121"
    top: "UpsamplingBilinear2dBackward122"
    convolution_param {
        num_output: 2
        group: 2
        kernel_size: 4
        stride: 2
        pad: 1
        weight_filler {
            type: "bilinear"
        }
        bias_term: false
    }
    param {
        lr_mult: 0
        decay_mult: 0
    }
}
layer {
    name: "Resample2dFunctionBackward123"
    type: "FlowWarp"
    bottom: "LeakyReluBackward18"
    bottom: "UpsamplingBilinear2dBackward122"
    top: "Resample2dFunctionBackward123"
}
layer {
    name: "CatBackward125"
    type: "Concat"
    bottom: "LeakyReluBackward13"
    bottom: "Resample2dFunctionBackward123"
    bottom: "UpsamplingBilinear2dBackward122"
    top: "CatBackward125"
    concat_param {
        axis: 1
    }
}
layer {
    name: "ConvNdBackward126"
    type: "Convolution"
    bottom: "CatBackward125"
    top: "ConvNdBackward126"
    convolution_param {
        num_output: 128
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward127"
    type: "ReLU"
    bottom: "ConvNdBackward126"
    top: "LeakyReluBackward127"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward128"
    type: "Convolution"
    bottom: "LeakyReluBackward127"
    top: "ConvNdBackward128"
    convolution_param {
        num_output: 128
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward129"
    type: "ReLU"
    bottom: "ConvNdBackward128"
    top: "LeakyReluBackward129"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward130"
    type: "Convolution"
    bottom: "LeakyReluBackward129"
    top: "ConvNdBackward130"
    convolution_param {
        num_output: 96
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward131"
    type: "ReLU"
    bottom: "ConvNdBackward130"
    top: "LeakyReluBackward131"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward132"
    type: "Convolution"
    bottom: "LeakyReluBackward131"
    top: "ConvNdBackward132"
    convolution_param {
        num_output: 64
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward133"
    type: "ReLU"
    bottom: "ConvNdBackward132"
    top: "LeakyReluBackward133"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward134"
    type: "Convolution"
    bottom: "LeakyReluBackward133"
    top: "ConvNdBackward134"
    convolution_param {
        num_output: 32
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward135"
    type: "ReLU"
    bottom: "ConvNdBackward134"
    top: "LeakyReluBackward135"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward136"
    type: "Convolution"
    bottom: "LeakyReluBackward135"
    top: "ConvNdBackward136"
    convolution_param {
        num_output: 2
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "UpsamplingBilinear2dBackward137"
    type: "Deconvolution"
    bottom: "ConvNdBackward136"
    top: "UpsamplingBilinear2dBackward137"
    convolution_param {
        num_output: 2
        group: 2
        kernel_size: 4
        stride: 2
        pad: 1
        weight_filler {
            type: "bilinear"
        }
        bias_term: false
    }
    param {
        lr_mult: 0
        decay_mult: 0
    }
}
layer {
    name: "Resample2dFunctionBackward138"
    type: "FlowWarp"
    bottom: "LeakyReluBackward8"
    bottom: "UpsamplingBilinear2dBackward137"
    top: "Resample2dFunctionBackward138"
}
layer {
    name: "CatBackward140"
    type: "Concat"
    bottom: "LeakyReluBackward4"
    bottom: "Resample2dFunctionBackward138"
    bottom: "UpsamplingBilinear2dBackward137"
    top: "CatBackward140"
    concat_param {
        axis: 1
    }
}
layer {
    name: "ConvNdBackward141"
    type: "Convolution"
    bottom: "CatBackward140"
    top: "ConvNdBackward141"
    convolution_param {
        num_output: 128
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward142"
    type: "ReLU"
    bottom: "ConvNdBackward141"
    top: "LeakyReluBackward142"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward143"
    type: "Convolution"
    bottom: "LeakyReluBackward142"
    top: "ConvNdBackward143"
    convolution_param {
        num_output: 128
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward144"
    type: "ReLU"
    bottom: "ConvNdBackward143"
    top: "LeakyReluBackward144"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward145"
    type: "Convolution"
    bottom: "LeakyReluBackward144"
    top: "ConvNdBackward145"
    convolution_param {
        num_output: 96
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward146"
    type: "ReLU"
    bottom: "ConvNdBackward145"
    top: "LeakyReluBackward146"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward147"
    type: "Convolution"
    bottom: "LeakyReluBackward146"
    top: "ConvNdBackward147"
    convolution_param {
        num_output: 64
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward148"
    type: "ReLU"
    bottom: "ConvNdBackward147"
    top: "LeakyReluBackward148"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward149"
    type: "Convolution"
    bottom: "LeakyReluBackward148"
    top: "ConvNdBackward149"
    convolution_param {
        num_output: 32
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward150"
    type: "ReLU"
    bottom: "ConvNdBackward149"
    top: "LeakyReluBackward150"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward152"
    type: "Convolution"
    bottom: "LeakyReluBackward150"
    top: "ConvNdBackward152"
    convolution_param {
        num_output: 2
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "CatBackward153"
    type: "Concat"
    bottom: "LeakyReluBackward150"
    bottom: "ConvNdBackward152"
    top: "CatBackward153"
    concat_param {
        axis: 1
    }
}
layer {
    name: "ConvNdBackward154"
    type: "Convolution"
    bottom: "CatBackward153"
    top: "ConvNdBackward154"
    convolution_param {
        num_output: 128
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: false
    }
}
layer {
    name: "LeakyReluBackward155"
    type: "ReLU"
    bottom: "ConvNdBackward154"
    top: "LeakyReluBackward155"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward156"
    type: "Convolution"
    bottom: "LeakyReluBackward155"
    top: "ConvNdBackward156"
    convolution_param {
        num_output: 128
        pad_h: 2
        pad_w: 2
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 2
        bias_term: false
    }
}
layer {
    name: "LeakyReluBackward157"
    type: "ReLU"
    bottom: "ConvNdBackward156"
    top: "LeakyReluBackward157"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward158"
    type: "Convolution"
    bottom: "LeakyReluBackward157"
    top: "ConvNdBackward158"
    convolution_param {
        num_output: 128
        pad_h: 4
        pad_w: 4
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 4
        bias_term: false
    }
}
layer {
    name: "LeakyReluBackward159"
    type: "ReLU"
    bottom: "ConvNdBackward158"
    top: "LeakyReluBackward159"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward160"
    type: "Convolution"
    bottom: "LeakyReluBackward159"
    top: "ConvNdBackward160"
    convolution_param {
        num_output: 96
        pad_h: 8
        pad_w: 8
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 8
        bias_term: false
    }
}
layer {
    name: "LeakyReluBackward161"
    type: "ReLU"
    bottom: "ConvNdBackward160"
    top: "LeakyReluBackward161"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward162"
    type: "Convolution"
    bottom: "LeakyReluBackward161"
    top: "ConvNdBackward162"
    convolution_param {
        num_output: 64
        pad_h: 16
        pad_w: 16
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 16
        bias_term: false
    }
}
layer {
    name: "LeakyReluBackward163"
    type: "ReLU"
    bottom: "ConvNdBackward162"
    top: "LeakyReluBackward163"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward164"
    type: "Convolution"
    bottom: "LeakyReluBackward163"
    top: "ConvNdBackward164"
    convolution_param {
        num_output: 32
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: false
    }
}
layer {
    name: "LeakyReluBackward165"
    type: "ReLU"
    bottom: "ConvNdBackward164"
    top: "LeakyReluBackward165"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward166"
    type: "Convolution"
    bottom: "LeakyReluBackward165"
    top: "ConvNdBackward166"
    convolution_param {
        num_output: 2
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: false
    }
}
layer {
    name: "AddBackward1168"
    type: "Eltwise"
    bottom: "ConvNdBackward166"
    bottom: "ConvNdBackward152"
    top: "AddBackward1168"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "UpsamplingBilinear2dBackward169"
    type: "Deconvolution"
    bottom: "AddBackward1168"
    top: "UpsamplingBilinear2dBackward169"
    convolution_param {
        num_output: 2
        group: 2
        kernel_size: 4
        stride: 2
        pad: 1
        weight_filler {
            type: "bilinear"
        }
        bias_term: false
    }
    param {
        lr_mult: 0
        decay_mult: 0
    }
}
layer {
    name: "MulBackward0170"
    type: "Eltwise"
    bottom: "UpsamplingBilinear2dBackward169"
    top: "MulBackward0170"
    eltwise_param {
        operation: MUL
        mulvalue: 0.5
    }
}
layer {
    name: "Resample2dFunctionBackward171"
    type: "FlowWarp"
    bottom: "data1"
    bottom: "MulBackward0170"
    top: "Resample2dFunctionBackward171"
}
layer {
    name: "ConvNdBackward172"
    type: "Convolution"
    bottom: "data1"
    top: "ConvNdBackward172"
    convolution_param {
        num_output: 64
        pad_h: 3
        pad_w: 3
        kernel_h: 7
        kernel_w: 7
        stride: 1
        dilation: 1
        bias_term: false
    }
}
layer {
    name: "Resample2dFunctionBackward174"
    type: "FlowWarp"
    bottom: "ConvNdBackward172"
    bottom: "MulBackward0170"
    top: "Resample2dFunctionBackward174"
}
layer {
    name: "ConvNdBackward175"
    type: "Convolution"
    bottom: "data2"
    top: "ConvNdBackward175"
    convolution_param {
        num_output: 16
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 2
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward176"
    type: "ReLU"
    bottom: "ConvNdBackward175"
    top: "LeakyReluBackward176"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward177"
    type: "Convolution"
    bottom: "LeakyReluBackward176"
    top: "ConvNdBackward177"
    convolution_param {
        num_output: 16
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward178"
    type: "ReLU"
    bottom: "ConvNdBackward177"
    top: "LeakyReluBackward178"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward179"
    type: "Convolution"
    bottom: "data1"
    top: "ConvNdBackward179"
    convolution_param {
        num_output: 16
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 2
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward180"
    type: "ReLU"
    bottom: "ConvNdBackward179"
    top: "LeakyReluBackward180"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward181"
    type: "Convolution"
    bottom: "LeakyReluBackward180"
    top: "ConvNdBackward181"
    convolution_param {
        num_output: 16
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward182"
    type: "ReLU"
    bottom: "ConvNdBackward181"
    top: "LeakyReluBackward182"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward184"
    type: "Convolution"
    bottom: "LeakyReluBackward178"
    top: "ConvNdBackward184"
    convolution_param {
        num_output: 32
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 2
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward185"
    type: "ReLU"
    bottom: "ConvNdBackward184"
    top: "LeakyReluBackward185"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward186"
    type: "Convolution"
    bottom: "LeakyReluBackward185"
    top: "ConvNdBackward186"
    convolution_param {
        num_output: 32
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward187"
    type: "ReLU"
    bottom: "ConvNdBackward186"
    top: "LeakyReluBackward187"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward189"
    type: "Convolution"
    bottom: "LeakyReluBackward182"
    top: "ConvNdBackward189"
    convolution_param {
        num_output: 32
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 2
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward190"
    type: "ReLU"
    bottom: "ConvNdBackward189"
    top: "LeakyReluBackward190"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward191"
    type: "Convolution"
    bottom: "LeakyReluBackward190"
    top: "ConvNdBackward191"
    convolution_param {
        num_output: 32
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward192"
    type: "ReLU"
    bottom: "ConvNdBackward191"
    top: "LeakyReluBackward192"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward194"
    type: "Convolution"
    bottom: "LeakyReluBackward187"
    top: "ConvNdBackward194"
    convolution_param {
        num_output: 64
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 2
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward195"
    type: "ReLU"
    bottom: "ConvNdBackward194"
    top: "LeakyReluBackward195"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward196"
    type: "Convolution"
    bottom: "LeakyReluBackward195"
    top: "ConvNdBackward196"
    convolution_param {
        num_output: 64
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward197"
    type: "ReLU"
    bottom: "ConvNdBackward196"
    top: "LeakyReluBackward197"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward199"
    type: "Convolution"
    bottom: "LeakyReluBackward192"
    top: "ConvNdBackward199"
    convolution_param {
        num_output: 64
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 2
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward200"
    type: "ReLU"
    bottom: "ConvNdBackward199"
    top: "LeakyReluBackward200"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward201"
    type: "Convolution"
    bottom: "LeakyReluBackward200"
    top: "ConvNdBackward201"
    convolution_param {
        num_output: 64
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward202"
    type: "ReLU"
    bottom: "ConvNdBackward201"
    top: "LeakyReluBackward202"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward204"
    type: "Convolution"
    bottom: "LeakyReluBackward197"
    top: "ConvNdBackward204"
    convolution_param {
        num_output: 96
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 2
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward205"
    type: "ReLU"
    bottom: "ConvNdBackward204"
    top: "LeakyReluBackward205"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward206"
    type: "Convolution"
    bottom: "LeakyReluBackward205"
    top: "ConvNdBackward206"
    convolution_param {
        num_output: 96
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward207"
    type: "ReLU"
    bottom: "ConvNdBackward206"
    top: "LeakyReluBackward207"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward209"
    type: "Convolution"
    bottom: "LeakyReluBackward202"
    top: "ConvNdBackward209"
    convolution_param {
        num_output: 96
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 2
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward210"
    type: "ReLU"
    bottom: "ConvNdBackward209"
    top: "LeakyReluBackward210"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward211"
    type: "Convolution"
    bottom: "LeakyReluBackward210"
    top: "ConvNdBackward211"
    convolution_param {
        num_output: 96
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward212"
    type: "ReLU"
    bottom: "ConvNdBackward211"
    top: "LeakyReluBackward212"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward214"
    type: "Convolution"
    bottom: "LeakyReluBackward207"
    top: "ConvNdBackward214"
    convolution_param {
        num_output: 128
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 2
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward215"
    type: "ReLU"
    bottom: "ConvNdBackward214"
    top: "LeakyReluBackward215"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward216"
    type: "Convolution"
    bottom: "LeakyReluBackward215"
    top: "ConvNdBackward216"
    convolution_param {
        num_output: 128
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward217"
    type: "ReLU"
    bottom: "ConvNdBackward216"
    top: "LeakyReluBackward217"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward219"
    type: "Convolution"
    bottom: "LeakyReluBackward212"
    top: "ConvNdBackward219"
    convolution_param {
        num_output: 128
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 2
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward220"
    type: "ReLU"
    bottom: "ConvNdBackward219"
    top: "LeakyReluBackward220"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward221"
    type: "Convolution"
    bottom: "LeakyReluBackward220"
    top: "ConvNdBackward221"
    convolution_param {
        num_output: 128
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward222"
    type: "ReLU"
    bottom: "ConvNdBackward221"
    top: "LeakyReluBackward222"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward224"
    type: "Convolution"
    bottom: "LeakyReluBackward217"
    top: "ConvNdBackward224"
    convolution_param {
        num_output: 192
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 2
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward225"
    type: "ReLU"
    bottom: "ConvNdBackward224"
    top: "LeakyReluBackward225"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward226"
    type: "Convolution"
    bottom: "LeakyReluBackward225"
    top: "ConvNdBackward226"
    convolution_param {
        num_output: 192
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward227"
    type: "ReLU"
    bottom: "ConvNdBackward226"
    top: "LeakyReluBackward227"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward229"
    type: "Convolution"
    bottom: "LeakyReluBackward222"
    top: "ConvNdBackward229"
    convolution_param {
        num_output: 192
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 2
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward230"
    type: "ReLU"
    bottom: "ConvNdBackward229"
    top: "LeakyReluBackward230"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward231"
    type: "Convolution"
    bottom: "LeakyReluBackward230"
    top: "ConvNdBackward231"
    convolution_param {
        num_output: 192
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward232"
    type: "ReLU"
    bottom: "ConvNdBackward231"
    top: "LeakyReluBackward232"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "IndexBackward234"
    type: "Slice"
    bottom: "LeakyReluBackward232"
    top: "IndexBackward234"
    top: "IndexBackward234_zcam"
    slice_param {
        axis: 1
        slice_point: 2
    }
}
layer {
    name: "SubBackward1236"
    type: "Eltwise"
    bottom: "IndexBackward234"
    bottom: "IndexBackward234"
    top: "SubBackward1236"
    eltwise_param {
        operation: SUM
        coeff: 1
        coeff: -1
    }
}
layer {
    name: "Resample2dFunctionBackward237"
    type: "FlowWarp"
    bottom: "LeakyReluBackward232"
    bottom: "SubBackward1236"
    top: "Resample2dFunctionBackward237"
}
layer {
    name: "CatBackward239"
    type: "Concat"
    bottom: "LeakyReluBackward227"
    bottom: "Resample2dFunctionBackward237"
    bottom: "SubBackward1236"
    top: "CatBackward239"
    concat_param {
        axis: 1
    }
}
layer {
    name: "ConvNdBackward240"
    type: "Convolution"
    bottom: "CatBackward239"
    top: "ConvNdBackward240"
    convolution_param {
        num_output: 128
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward241"
    type: "ReLU"
    bottom: "ConvNdBackward240"
    top: "LeakyReluBackward241"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward242"
    type: "Convolution"
    bottom: "LeakyReluBackward241"
    top: "ConvNdBackward242"
    convolution_param {
        num_output: 128
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward243"
    type: "ReLU"
    bottom: "ConvNdBackward242"
    top: "LeakyReluBackward243"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward244"
    type: "Convolution"
    bottom: "LeakyReluBackward243"
    top: "ConvNdBackward244"
    convolution_param {
        num_output: 96
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward245"
    type: "ReLU"
    bottom: "ConvNdBackward244"
    top: "LeakyReluBackward245"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward246"
    type: "Convolution"
    bottom: "LeakyReluBackward245"
    top: "ConvNdBackward246"
    convolution_param {
        num_output: 64
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward247"
    type: "ReLU"
    bottom: "ConvNdBackward246"
    top: "LeakyReluBackward247"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward248"
    type: "Convolution"
    bottom: "LeakyReluBackward247"
    top: "ConvNdBackward248"
    convolution_param {
        num_output: 32
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward249"
    type: "ReLU"
    bottom: "ConvNdBackward248"
    top: "LeakyReluBackward249"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward250"
    type: "Convolution"
    bottom: "LeakyReluBackward249"
    top: "ConvNdBackward250"
    convolution_param {
        num_output: 2
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "UpsamplingBilinear2dBackward251"
    type: "Deconvolution"
    bottom: "ConvNdBackward250"
    top: "UpsamplingBilinear2dBackward251"
    convolution_param {
        num_output: 2
        group: 2
        kernel_size: 4
        stride: 2
        pad: 1
        weight_filler {
            type: "bilinear"
        }
        bias_term: false
    }
    param {
        lr_mult: 0
        decay_mult: 0
    }
}
layer {
    name: "Resample2dFunctionBackward252"
    type: "FlowWarp"
    bottom: "LeakyReluBackward222"
    bottom: "UpsamplingBilinear2dBackward251"
    top: "Resample2dFunctionBackward252"
}
layer {
    name: "CatBackward254"
    type: "Concat"
    bottom: "LeakyReluBackward217"
    bottom: "Resample2dFunctionBackward252"
    bottom: "UpsamplingBilinear2dBackward251"
    top: "CatBackward254"
    concat_param {
        axis: 1
    }
}
layer {
    name: "ConvNdBackward255"
    type: "Convolution"
    bottom: "CatBackward254"
    top: "ConvNdBackward255"
    convolution_param {
        num_output: 128
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward256"
    type: "ReLU"
    bottom: "ConvNdBackward255"
    top: "LeakyReluBackward256"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward257"
    type: "Convolution"
    bottom: "LeakyReluBackward256"
    top: "ConvNdBackward257"
    convolution_param {
        num_output: 128
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward258"
    type: "ReLU"
    bottom: "ConvNdBackward257"
    top: "LeakyReluBackward258"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward259"
    type: "Convolution"
    bottom: "LeakyReluBackward258"
    top: "ConvNdBackward259"
    convolution_param {
        num_output: 96
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward260"
    type: "ReLU"
    bottom: "ConvNdBackward259"
    top: "LeakyReluBackward260"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward261"
    type: "Convolution"
    bottom: "LeakyReluBackward260"
    top: "ConvNdBackward261"
    convolution_param {
        num_output: 64
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward262"
    type: "ReLU"
    bottom: "ConvNdBackward261"
    top: "LeakyReluBackward262"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward263"
    type: "Convolution"
    bottom: "LeakyReluBackward262"
    top: "ConvNdBackward263"
    convolution_param {
        num_output: 32
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward264"
    type: "ReLU"
    bottom: "ConvNdBackward263"
    top: "LeakyReluBackward264"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward265"
    type: "Convolution"
    bottom: "LeakyReluBackward264"
    top: "ConvNdBackward265"
    convolution_param {
        num_output: 2
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "UpsamplingBilinear2dBackward266"
    type: "Deconvolution"
    bottom: "ConvNdBackward265"
    top: "UpsamplingBilinear2dBackward266"
    convolution_param {
        num_output: 2
        group: 2
        kernel_size: 4
        stride: 2
        pad: 1
        weight_filler {
            type: "bilinear"
        }
        bias_term: false
    }
    param {
        lr_mult: 0
        decay_mult: 0
    }
}
layer {
    name: "Resample2dFunctionBackward267"
    type: "FlowWarp"
    bottom: "LeakyReluBackward212"
    bottom: "UpsamplingBilinear2dBackward266"
    top: "Resample2dFunctionBackward267"
}
layer {
    name: "CatBackward269"
    type: "Concat"
    bottom: "LeakyReluBackward207"
    bottom: "Resample2dFunctionBackward267"
    bottom: "UpsamplingBilinear2dBackward266"
    top: "CatBackward269"
    concat_param {
        axis: 1
    }
}
layer {
    name: "ConvNdBackward270"
    type: "Convolution"
    bottom: "CatBackward269"
    top: "ConvNdBackward270"
    convolution_param {
        num_output: 128
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward271"
    type: "ReLU"
    bottom: "ConvNdBackward270"
    top: "LeakyReluBackward271"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward272"
    type: "Convolution"
    bottom: "LeakyReluBackward271"
    top: "ConvNdBackward272"
    convolution_param {
        num_output: 128
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward273"
    type: "ReLU"
    bottom: "ConvNdBackward272"
    top: "LeakyReluBackward273"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward274"
    type: "Convolution"
    bottom: "LeakyReluBackward273"
    top: "ConvNdBackward274"
    convolution_param {
        num_output: 96
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward275"
    type: "ReLU"
    bottom: "ConvNdBackward274"
    top: "LeakyReluBackward275"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward276"
    type: "Convolution"
    bottom: "LeakyReluBackward275"
    top: "ConvNdBackward276"
    convolution_param {
        num_output: 64
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward277"
    type: "ReLU"
    bottom: "ConvNdBackward276"
    top: "LeakyReluBackward277"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward278"
    type: "Convolution"
    bottom: "LeakyReluBackward277"
    top: "ConvNdBackward278"
    convolution_param {
        num_output: 32
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward279"
    type: "ReLU"
    bottom: "ConvNdBackward278"
    top: "LeakyReluBackward279"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward280"
    type: "Convolution"
    bottom: "LeakyReluBackward279"
    top: "ConvNdBackward280"
    convolution_param {
        num_output: 2
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "UpsamplingBilinear2dBackward281"
    type: "Deconvolution"
    bottom: "ConvNdBackward280"
    top: "UpsamplingBilinear2dBackward281"
    convolution_param {
        num_output: 2
        group: 2
        kernel_size: 4
        stride: 2
        pad: 1
        weight_filler {
            type: "bilinear"
        }
        bias_term: false
    }
    param {
        lr_mult: 0
        decay_mult: 0
    }
}
layer {
    name: "Resample2dFunctionBackward282"
    type: "FlowWarp"
    bottom: "LeakyReluBackward202"
    bottom: "UpsamplingBilinear2dBackward281"
    top: "Resample2dFunctionBackward282"
}
layer {
    name: "CatBackward284"
    type: "Concat"
    bottom: "LeakyReluBackward197"
    bottom: "Resample2dFunctionBackward282"
    bottom: "UpsamplingBilinear2dBackward281"
    top: "CatBackward284"
    concat_param {
        axis: 1
    }
}
layer {
    name: "ConvNdBackward285"
    type: "Convolution"
    bottom: "CatBackward284"
    top: "ConvNdBackward285"
    convolution_param {
        num_output: 128
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward286"
    type: "ReLU"
    bottom: "ConvNdBackward285"
    top: "LeakyReluBackward286"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward287"
    type: "Convolution"
    bottom: "LeakyReluBackward286"
    top: "ConvNdBackward287"
    convolution_param {
        num_output: 128
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward288"
    type: "ReLU"
    bottom: "ConvNdBackward287"
    top: "LeakyReluBackward288"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward289"
    type: "Convolution"
    bottom: "LeakyReluBackward288"
    top: "ConvNdBackward289"
    convolution_param {
        num_output: 96
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward290"
    type: "ReLU"
    bottom: "ConvNdBackward289"
    top: "LeakyReluBackward290"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward291"
    type: "Convolution"
    bottom: "LeakyReluBackward290"
    top: "ConvNdBackward291"
    convolution_param {
        num_output: 64
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward292"
    type: "ReLU"
    bottom: "ConvNdBackward291"
    top: "LeakyReluBackward292"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward293"
    type: "Convolution"
    bottom: "LeakyReluBackward292"
    top: "ConvNdBackward293"
    convolution_param {
        num_output: 32
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward294"
    type: "ReLU"
    bottom: "ConvNdBackward293"
    top: "LeakyReluBackward294"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward295"
    type: "Convolution"
    bottom: "LeakyReluBackward294"
    top: "ConvNdBackward295"
    convolution_param {
        num_output: 2
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "UpsamplingBilinear2dBackward296"
    type: "Deconvolution"
    bottom: "ConvNdBackward295"
    top: "UpsamplingBilinear2dBackward296"
    convolution_param {
        num_output: 2
        group: 2
        kernel_size: 4
        stride: 2
        pad: 1
        weight_filler {
            type: "bilinear"
        }
        bias_term: false
    }
    param {
        lr_mult: 0
        decay_mult: 0
    }
}
layer {
    name: "Resample2dFunctionBackward297"
    type: "FlowWarp"
    bottom: "LeakyReluBackward192"
    bottom: "UpsamplingBilinear2dBackward296"
    top: "Resample2dFunctionBackward297"
}
layer {
    name: "CatBackward299"
    type: "Concat"
    bottom: "LeakyReluBackward187"
    bottom: "Resample2dFunctionBackward297"
    bottom: "UpsamplingBilinear2dBackward296"
    top: "CatBackward299"
    concat_param {
        axis: 1
    }
}
layer {
    name: "ConvNdBackward300"
    type: "Convolution"
    bottom: "CatBackward299"
    top: "ConvNdBackward300"
    convolution_param {
        num_output: 128
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward301"
    type: "ReLU"
    bottom: "ConvNdBackward300"
    top: "LeakyReluBackward301"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward302"
    type: "Convolution"
    bottom: "LeakyReluBackward301"
    top: "ConvNdBackward302"
    convolution_param {
        num_output: 128
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward303"
    type: "ReLU"
    bottom: "ConvNdBackward302"
    top: "LeakyReluBackward303"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward304"
    type: "Convolution"
    bottom: "LeakyReluBackward303"
    top: "ConvNdBackward304"
    convolution_param {
        num_output: 96
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward305"
    type: "ReLU"
    bottom: "ConvNdBackward304"
    top: "LeakyReluBackward305"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward306"
    type: "Convolution"
    bottom: "LeakyReluBackward305"
    top: "ConvNdBackward306"
    convolution_param {
        num_output: 64
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward307"
    type: "ReLU"
    bottom: "ConvNdBackward306"
    top: "LeakyReluBackward307"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward308"
    type: "Convolution"
    bottom: "LeakyReluBackward307"
    top: "ConvNdBackward308"
    convolution_param {
        num_output: 32
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward309"
    type: "ReLU"
    bottom: "ConvNdBackward308"
    top: "LeakyReluBackward309"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward310"
    type: "Convolution"
    bottom: "LeakyReluBackward309"
    top: "ConvNdBackward310"
    convolution_param {
        num_output: 2
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "UpsamplingBilinear2dBackward311"
    type: "Deconvolution"
    bottom: "ConvNdBackward310"
    top: "UpsamplingBilinear2dBackward311"
    convolution_param {
        num_output: 2
        group: 2
        kernel_size: 4
        stride: 2
        pad: 1
        weight_filler {
            type: "bilinear"
        }
        bias_term: false
    }
    param {
        lr_mult: 0
        decay_mult: 0
    }
}
layer {
    name: "Resample2dFunctionBackward312"
    type: "FlowWarp"
    bottom: "LeakyReluBackward182"
    bottom: "UpsamplingBilinear2dBackward311"
    top: "Resample2dFunctionBackward312"
}
layer {
    name: "CatBackward314"
    type: "Concat"
    bottom: "LeakyReluBackward178"
    bottom: "Resample2dFunctionBackward312"
    bottom: "UpsamplingBilinear2dBackward311"
    top: "CatBackward314"
    concat_param {
        axis: 1
    }
}
layer {
    name: "ConvNdBackward315"
    type: "Convolution"
    bottom: "CatBackward314"
    top: "ConvNdBackward315"
    convolution_param {
        num_output: 128
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward316"
    type: "ReLU"
    bottom: "ConvNdBackward315"
    top: "LeakyReluBackward316"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward317"
    type: "Convolution"
    bottom: "LeakyReluBackward316"
    top: "ConvNdBackward317"
    convolution_param {
        num_output: 128
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward318"
    type: "ReLU"
    bottom: "ConvNdBackward317"
    top: "LeakyReluBackward318"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward319"
    type: "Convolution"
    bottom: "LeakyReluBackward318"
    top: "ConvNdBackward319"
    convolution_param {
        num_output: 96
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward320"
    type: "ReLU"
    bottom: "ConvNdBackward319"
    top: "LeakyReluBackward320"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward321"
    type: "Convolution"
    bottom: "LeakyReluBackward320"
    top: "ConvNdBackward321"
    convolution_param {
        num_output: 64
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward322"
    type: "ReLU"
    bottom: "ConvNdBackward321"
    top: "LeakyReluBackward322"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward323"
    type: "Convolution"
    bottom: "LeakyReluBackward322"
    top: "ConvNdBackward323"
    convolution_param {
        num_output: 32
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward324"
    type: "ReLU"
    bottom: "ConvNdBackward323"
    top: "LeakyReluBackward324"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward326"
    type: "Convolution"
    bottom: "LeakyReluBackward324"
    top: "ConvNdBackward326"
    convolution_param {
        num_output: 2
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "CatBackward327"
    type: "Concat"
    bottom: "LeakyReluBackward324"
    bottom: "ConvNdBackward326"
    top: "CatBackward327"
    concat_param {
        axis: 1
    }
}
layer {
    name: "ConvNdBackward328"
    type: "Convolution"
    bottom: "CatBackward327"
    top: "ConvNdBackward328"
    convolution_param {
        num_output: 128
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: false
    }
}
layer {
    name: "LeakyReluBackward329"
    type: "ReLU"
    bottom: "ConvNdBackward328"
    top: "LeakyReluBackward329"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward330"
    type: "Convolution"
    bottom: "LeakyReluBackward329"
    top: "ConvNdBackward330"
    convolution_param {
        num_output: 128
        pad_h: 2
        pad_w: 2
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 2
        bias_term: false
    }
}
layer {
    name: "LeakyReluBackward331"
    type: "ReLU"
    bottom: "ConvNdBackward330"
    top: "LeakyReluBackward331"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward332"
    type: "Convolution"
    bottom: "LeakyReluBackward331"
    top: "ConvNdBackward332"
    convolution_param {
        num_output: 128
        pad_h: 4
        pad_w: 4
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 4
        bias_term: false
    }
}
layer {
    name: "LeakyReluBackward333"
    type: "ReLU"
    bottom: "ConvNdBackward332"
    top: "LeakyReluBackward333"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward334"
    type: "Convolution"
    bottom: "LeakyReluBackward333"
    top: "ConvNdBackward334"
    convolution_param {
        num_output: 96
        pad_h: 8
        pad_w: 8
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 8
        bias_term: false
    }
}
layer {
    name: "LeakyReluBackward335"
    type: "ReLU"
    bottom: "ConvNdBackward334"
    top: "LeakyReluBackward335"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward336"
    type: "Convolution"
    bottom: "LeakyReluBackward335"
    top: "ConvNdBackward336"
    convolution_param {
        num_output: 64
        pad_h: 16
        pad_w: 16
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 16
        bias_term: false
    }
}
layer {
    name: "LeakyReluBackward337"
    type: "ReLU"
    bottom: "ConvNdBackward336"
    top: "LeakyReluBackward337"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward338"
    type: "Convolution"
    bottom: "LeakyReluBackward337"
    top: "ConvNdBackward338"
    convolution_param {
        num_output: 32
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: false
    }
}
layer {
    name: "LeakyReluBackward339"
    type: "ReLU"
    bottom: "ConvNdBackward338"
    top: "LeakyReluBackward339"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward340"
    type: "Convolution"
    bottom: "LeakyReluBackward339"
    top: "ConvNdBackward340"
    convolution_param {
        num_output: 2
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: false
    }
}
layer {
    name: "AddBackward1342"
    type: "Eltwise"
    bottom: "ConvNdBackward340"
    bottom: "ConvNdBackward326"
    top: "AddBackward1342"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "UpsamplingBilinear2dBackward343"
    type: "Deconvolution"
    bottom: "AddBackward1342"
    top: "UpsamplingBilinear2dBackward343"
    convolution_param {
        num_output: 2
        group: 2
        kernel_size: 4
        stride: 2
        pad: 1
        weight_filler {
            type: "bilinear"
        }
        bias_term: false
    }
    param {
        lr_mult: 0
        decay_mult: 0
    }
}
layer {
    name: "MulBackward0344"
    type: "Eltwise"
    bottom: "UpsamplingBilinear2dBackward343"
    top: "MulBackward0344"
    eltwise_param {
        operation: MUL
        mulvalue: 0.5
    }
}
layer {
    name: "Resample2dFunctionBackward345"
    type: "FlowWarp"
    bottom: "data2"
    bottom: "MulBackward0344"
    top: "Resample2dFunctionBackward345"
}
layer {
    name: "ConvNdBackward346"
    type: "Convolution"
    bottom: "data2"
    top: "ConvNdBackward346"
    convolution_param {
        num_output: 64
        pad_h: 3
        pad_w: 3
        kernel_h: 7
        kernel_w: 7
        stride: 1
        dilation: 1
        bias_term: false
    }
}
layer {
    name: "Resample2dFunctionBackward348"
    type: "FlowWarp"
    bottom: "ConvNdBackward346"
    bottom: "MulBackward0344"
    top: "Resample2dFunctionBackward348"
}
layer {
    name: "CatBackward349"
    type: "Concat"
    bottom: "Resample2dFunctionBackward171"
    bottom: "Resample2dFunctionBackward174"
    bottom: "Resample2dFunctionBackward345"
    bottom: "Resample2dFunctionBackward348"
    top: "CatBackward349"
    concat_param {
        axis: 1
    }
}
layer {
    name: "PReLUBackward350"
    type: "PReLU"
    bottom: "CatBackward349"
    top: "PReLUBackward350"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.788865
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward351"
    type: "Convolution"
    bottom: "PReLUBackward350"
    top: "ConvNdBackward351"
    convolution_param {
        num_output: 32
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "PReLUBackward352"
    type: "PReLU"
    bottom: "ConvNdBackward351"
    top: "PReLUBackward352"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.1292519
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward353"
    type: "Convolution"
    bottom: "PReLUBackward352"
    top: "ConvNdBackward353"
    convolution_param {
        num_output: 32
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "PReLUBackward354"
    type: "PReLU"
    bottom: "ConvNdBackward353"
    top: "PReLUBackward354"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.12573308
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward355"
    type: "Convolution"
    bottom: "PReLUBackward354"
    top: "ConvNdBackward355"
    convolution_param {
        num_output: 64
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 2
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "PReLUBackward356"
    type: "PReLU"
    bottom: "ConvNdBackward355"
    top: "PReLUBackward356"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.080189854
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward357"
    type: "Convolution"
    bottom: "PReLUBackward356"
    top: "ConvNdBackward357"
    convolution_param {
        num_output: 64
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "PReLUBackward358"
    type: "PReLU"
    bottom: "ConvNdBackward357"
    top: "PReLUBackward358"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.07449101
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward359"
    type: "Convolution"
    bottom: "PReLUBackward358"
    top: "ConvNdBackward359"
    convolution_param {
        num_output: 96
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 2
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "PReLUBackward360"
    type: "PReLU"
    bottom: "ConvNdBackward359"
    top: "PReLUBackward360"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.061320506
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward361"
    type: "Convolution"
    bottom: "PReLUBackward360"
    top: "ConvNdBackward361"
    convolution_param {
        num_output: 96
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "PReLUBackward362"
    type: "PReLU"
    bottom: "ConvNdBackward361"
    top: "PReLUBackward362"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.051958483
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward363"
    type: "Convolution"
    bottom: "PReLUBackward362"
    top: "ConvNdBackward363"
    convolution_param {
        num_output: 96
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "PReLUBackward364"
    type: "PReLU"
    bottom: "ConvNdBackward363"
    top: "PReLUBackward364"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.023155011
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward365"
    type: "Convolution"
    bottom: "PReLUBackward364"
    top: "ConvNdBackward365"
    convolution_param {
        num_output: 96
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "AddBackward1367"
    type: "Eltwise"
    bottom: "ConvNdBackward365"
    bottom: "ConvNdBackward361"
    top: "AddBackward1367"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "PReLUBackward369"
    type: "PReLU"
    bottom: "ConvNdBackward357"
    top: "PReLUBackward369"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.107141905
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward370"
    type: "Convolution"
    bottom: "PReLUBackward369"
    top: "ConvNdBackward370"
    convolution_param {
        num_output: 64
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "PReLUBackward371"
    type: "PReLU"
    bottom: "ConvNdBackward370"
    top: "PReLUBackward371"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.019786308
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward372"
    type: "Convolution"
    bottom: "PReLUBackward371"
    top: "ConvNdBackward372"
    convolution_param {
        num_output: 64
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "AddBackward1374"
    type: "Eltwise"
    bottom: "ConvNdBackward372"
    bottom: "ConvNdBackward357"
    top: "AddBackward1374"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "PReLUBackward376"
    type: "PReLU"
    bottom: "ConvNdBackward353"
    top: "PReLUBackward376"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.89768916
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward377"
    type: "Convolution"
    bottom: "PReLUBackward376"
    top: "ConvNdBackward377"
    convolution_param {
        num_output: 32
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "PReLUBackward378"
    type: "PReLU"
    bottom: "ConvNdBackward377"
    top: "PReLUBackward378"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.82034045
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward379"
    type: "Convolution"
    bottom: "PReLUBackward378"
    top: "ConvNdBackward379"
    convolution_param {
        num_output: 32
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "AddBackward1381"
    type: "Eltwise"
    bottom: "ConvNdBackward379"
    bottom: "ConvNdBackward353"
    top: "AddBackward1381"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "PReLUBackward382"
    type: "PReLU"
    bottom: "AddBackward1381"
    top: "PReLUBackward382"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.15072164
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward383"
    type: "Convolution"
    bottom: "PReLUBackward382"
    top: "ConvNdBackward383"
    convolution_param {
        num_output: 64
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 2
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "PReLUBackward384"
    type: "PReLU"
    bottom: "ConvNdBackward383"
    top: "PReLUBackward384"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.062501654
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward385"
    type: "Convolution"
    bottom: "PReLUBackward384"
    top: "ConvNdBackward385"
    convolution_param {
        num_output: 64
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "AddBackward1386"
    type: "Eltwise"
    bottom: "AddBackward1374"
    bottom: "ConvNdBackward385"
    top: "AddBackward1386"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "PReLUBackward387"
    type: "PReLU"
    bottom: "AddBackward1386"
    top: "PReLUBackward387"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.007465811
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward388"
    type: "Convolution"
    bottom: "PReLUBackward387"
    top: "ConvNdBackward388"
    convolution_param {
        num_output: 96
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 2
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "PReLUBackward389"
    type: "PReLU"
    bottom: "ConvNdBackward388"
    top: "PReLUBackward389"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.013434936
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward390"
    type: "Convolution"
    bottom: "PReLUBackward389"
    top: "ConvNdBackward390"
    convolution_param {
        num_output: 96
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "AddBackward1391"
    type: "Eltwise"
    bottom: "AddBackward1367"
    bottom: "ConvNdBackward390"
    top: "AddBackward1391"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "PReLUBackward392"
    type: "PReLU"
    bottom: "AddBackward1391"
    top: "PReLUBackward392"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.05479647
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward393"
    type: "Convolution"
    bottom: "PReLUBackward392"
    top: "ConvNdBackward393"
    convolution_param {
        num_output: 96
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "PReLUBackward394"
    type: "PReLU"
    bottom: "ConvNdBackward393"
    top: "PReLUBackward394"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.018806437
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward395"
    type: "Convolution"
    bottom: "PReLUBackward394"
    top: "ConvNdBackward395"
    convolution_param {
        num_output: 96
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "AddBackward1397"
    type: "Eltwise"
    bottom: "ConvNdBackward395"
    bottom: "AddBackward1391"
    top: "AddBackward1397"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "PReLUBackward399"
    type: "PReLU"
    bottom: "AddBackward1386"
    top: "PReLUBackward399"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.066501625
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward400"
    type: "Convolution"
    bottom: "PReLUBackward399"
    top: "ConvNdBackward400"
    convolution_param {
        num_output: 64
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "PReLUBackward401"
    type: "PReLU"
    bottom: "ConvNdBackward400"
    top: "PReLUBackward401"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.02395215
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward402"
    type: "Convolution"
    bottom: "PReLUBackward401"
    top: "ConvNdBackward402"
    convolution_param {
        num_output: 64
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "AddBackward1404"
    type: "Eltwise"
    bottom: "ConvNdBackward402"
    bottom: "AddBackward1386"
    top: "AddBackward1404"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "PReLUBackward406"
    type: "PReLU"
    bottom: "AddBackward1381"
    top: "PReLUBackward406"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.17068893
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward407"
    type: "Convolution"
    bottom: "PReLUBackward406"
    top: "ConvNdBackward407"
    convolution_param {
        num_output: 32
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "PReLUBackward408"
    type: "PReLU"
    bottom: "ConvNdBackward407"
    top: "PReLUBackward408"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.010727377
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward409"
    type: "Convolution"
    bottom: "PReLUBackward408"
    top: "ConvNdBackward409"
    convolution_param {
        num_output: 32
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "AddBackward1411"
    type: "Eltwise"
    bottom: "ConvNdBackward409"
    bottom: "AddBackward1381"
    top: "AddBackward1411"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "PReLUBackward412"
    type: "PReLU"
    bottom: "AddBackward1411"
    top: "PReLUBackward412"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.29481852
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward413"
    type: "Convolution"
    bottom: "PReLUBackward412"
    top: "ConvNdBackward413"
    convolution_param {
        num_output: 64
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 2
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "PReLUBackward414"
    type: "PReLU"
    bottom: "ConvNdBackward413"
    top: "PReLUBackward414"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.10870449
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward415"
    type: "Convolution"
    bottom: "PReLUBackward414"
    top: "ConvNdBackward415"
    convolution_param {
        num_output: 64
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "AddBackward1416"
    type: "Eltwise"
    bottom: "AddBackward1404"
    bottom: "ConvNdBackward415"
    top: "AddBackward1416"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "PReLUBackward417"
    type: "PReLU"
    bottom: "AddBackward1416"
    top: "PReLUBackward417"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.06190768
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward418"
    type: "Convolution"
    bottom: "PReLUBackward417"
    top: "ConvNdBackward418"
    convolution_param {
        num_output: 96
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 2
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "PReLUBackward419"
    type: "PReLU"
    bottom: "ConvNdBackward418"
    top: "PReLUBackward419"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.06350038
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward420"
    type: "Convolution"
    bottom: "PReLUBackward419"
    top: "ConvNdBackward420"
    convolution_param {
        num_output: 96
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "AddBackward1421"
    type: "Eltwise"
    bottom: "AddBackward1397"
    bottom: "ConvNdBackward420"
    top: "AddBackward1421"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "PReLUBackward422"
    type: "PReLU"
    bottom: "AddBackward1421"
    top: "PReLUBackward422"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.105041154
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward423"
    type: "Convolution"
    bottom: "PReLUBackward422"
    top: "ConvNdBackward423"
    convolution_param {
        num_output: 96
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "PReLUBackward424"
    type: "PReLU"
    bottom: "ConvNdBackward423"
    top: "PReLUBackward424"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.0369669
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward425"
    type: "Convolution"
    bottom: "PReLUBackward424"
    top: "ConvNdBackward425"
    convolution_param {
        num_output: 96
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "AddBackward1427"
    type: "Eltwise"
    bottom: "ConvNdBackward425"
    bottom: "AddBackward1421"
    top: "AddBackward1427"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "PReLUBackward428"
    type: "PReLU"
    bottom: "AddBackward1427"
    top: "PReLUBackward428"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.19046164
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward429"
    type: "Convolution"
    bottom: "PReLUBackward428"
    top: "ConvNdBackward429"
    convolution_param {
        num_output: 96
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "PReLUBackward430"
    type: "PReLU"
    bottom: "ConvNdBackward429"
    top: "PReLUBackward430"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.07204026
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward431"
    type: "Convolution"
    bottom: "PReLUBackward430"
    top: "ConvNdBackward431"
    convolution_param {
        num_output: 96
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "AddBackward1433"
    type: "Eltwise"
    bottom: "ConvNdBackward431"
    bottom: "AddBackward1427"
    top: "AddBackward1433"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "PReLUBackward434"
    type: "PReLU"
    bottom: "AddBackward1433"
    top: "PReLUBackward434"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.23906207
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward435"
    type: "Convolution"
    bottom: "PReLUBackward434"
    top: "ConvNdBackward435"
    convolution_param {
        num_output: 96
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "PReLUBackward436"
    type: "PReLU"
    bottom: "ConvNdBackward435"
    top: "PReLUBackward436"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.070694074
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward437"
    type: "Convolution"
    bottom: "PReLUBackward436"
    top: "ConvNdBackward437"
    convolution_param {
        num_output: 96
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "AddBackward1439"
    type: "Eltwise"
    bottom: "ConvNdBackward437"
    bottom: "AddBackward1433"
    top: "AddBackward1439"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "UpsamplingBilinear2dBackward440"
    type: "Deconvolution"
    bottom: "AddBackward1439"
    top: "UpsamplingBilinear2dBackward440"
    convolution_param {
        num_output: 96
        group: 96
        kernel_size: 4
        stride: 2
        pad: 1
        weight_filler {
            type: "bilinear"
        }
        bias_term: false
    }
    param {
        lr_mult: 0
        decay_mult: 0
    }
}
layer {
    name: "PReLUBackward441"
    type: "PReLU"
    bottom: "UpsamplingBilinear2dBackward440"
    top: "PReLUBackward441"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.03928604
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward442"
    type: "Convolution"
    bottom: "PReLUBackward441"
    top: "ConvNdBackward442"
    convolution_param {
        num_output: 64
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "PReLUBackward443"
    type: "PReLU"
    bottom: "ConvNdBackward442"
    top: "PReLUBackward443"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.00472327
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward444"
    type: "Convolution"
    bottom: "PReLUBackward443"
    top: "ConvNdBackward444"
    convolution_param {
        num_output: 64
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "UpsamplingBilinear2dBackward446"
    type: "Deconvolution"
    bottom: "AddBackward1433"
    top: "UpsamplingBilinear2dBackward446"
    convolution_param {
        num_output: 96
        group: 96
        kernel_size: 4
        stride: 2
        pad: 1
        weight_filler {
            type: "bilinear"
        }
        bias_term: false
    }
    param {
        lr_mult: 0
        decay_mult: 0
    }
}
layer {
    name: "PReLUBackward447"
    type: "PReLU"
    bottom: "UpsamplingBilinear2dBackward446"
    top: "PReLUBackward447"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.1336504
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward448"
    type: "Convolution"
    bottom: "PReLUBackward447"
    top: "ConvNdBackward448"
    convolution_param {
        num_output: 64
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "PReLUBackward449"
    type: "PReLU"
    bottom: "ConvNdBackward448"
    top: "PReLUBackward449"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.01500565
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward450"
    type: "Convolution"
    bottom: "PReLUBackward449"
    top: "ConvNdBackward450"
    convolution_param {
        num_output: 64
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "UpsamplingBilinear2dBackward452"
    type: "Deconvolution"
    bottom: "AddBackward1427"
    top: "UpsamplingBilinear2dBackward452"
    convolution_param {
        num_output: 96
        group: 96
        kernel_size: 4
        stride: 2
        pad: 1
        weight_filler {
            type: "bilinear"
        }
        bias_term: false
    }
    param {
        lr_mult: 0
        decay_mult: 0
    }
}
layer {
    name: "PReLUBackward453"
    type: "PReLU"
    bottom: "UpsamplingBilinear2dBackward452"
    top: "PReLUBackward453"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.14253029
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward454"
    type: "Convolution"
    bottom: "PReLUBackward453"
    top: "ConvNdBackward454"
    convolution_param {
        num_output: 64
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "PReLUBackward455"
    type: "PReLU"
    bottom: "ConvNdBackward454"
    top: "PReLUBackward455"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.020765519
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward456"
    type: "Convolution"
    bottom: "PReLUBackward455"
    top: "ConvNdBackward456"
    convolution_param {
        num_output: 64
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "PReLUBackward458"
    type: "PReLU"
    bottom: "AddBackward1416"
    top: "PReLUBackward458"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.27996758
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward459"
    type: "Convolution"
    bottom: "PReLUBackward458"
    top: "ConvNdBackward459"
    convolution_param {
        num_output: 64
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "PReLUBackward460"
    type: "PReLU"
    bottom: "ConvNdBackward459"
    top: "PReLUBackward460"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.17971475
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward461"
    type: "Convolution"
    bottom: "PReLUBackward460"
    top: "ConvNdBackward461"
    convolution_param {
        num_output: 64
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "AddBackward1463"
    type: "Eltwise"
    bottom: "ConvNdBackward461"
    bottom: "AddBackward1416"
    top: "AddBackward1463"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "AddBackward1464"
    type: "Eltwise"
    bottom: "ConvNdBackward456"
    bottom: "AddBackward1463"
    top: "AddBackward1464"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "PReLUBackward465"
    type: "PReLU"
    bottom: "AddBackward1464"
    top: "PReLUBackward465"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.20819548
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward466"
    type: "Convolution"
    bottom: "PReLUBackward465"
    top: "ConvNdBackward466"
    convolution_param {
        num_output: 64
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "PReLUBackward467"
    type: "PReLU"
    bottom: "ConvNdBackward466"
    top: "PReLUBackward467"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.05117616
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward468"
    type: "Convolution"
    bottom: "PReLUBackward467"
    top: "ConvNdBackward468"
    convolution_param {
        num_output: 64
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "AddBackward1470"
    type: "Eltwise"
    bottom: "ConvNdBackward468"
    bottom: "AddBackward1464"
    top: "AddBackward1470"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "AddBackward1471"
    type: "Eltwise"
    bottom: "ConvNdBackward450"
    bottom: "AddBackward1470"
    top: "AddBackward1471"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "PReLUBackward472"
    type: "PReLU"
    bottom: "AddBackward1471"
    top: "PReLUBackward472"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.31337413
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward473"
    type: "Convolution"
    bottom: "PReLUBackward472"
    top: "ConvNdBackward473"
    convolution_param {
        num_output: 64
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "PReLUBackward474"
    type: "PReLU"
    bottom: "ConvNdBackward473"
    top: "PReLUBackward474"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.016940834
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward475"
    type: "Convolution"
    bottom: "PReLUBackward474"
    top: "ConvNdBackward475"
    convolution_param {
        num_output: 64
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "AddBackward1477"
    type: "Eltwise"
    bottom: "ConvNdBackward475"
    bottom: "AddBackward1471"
    top: "AddBackward1477"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "AddBackward1478"
    type: "Eltwise"
    bottom: "ConvNdBackward444"
    bottom: "AddBackward1477"
    top: "AddBackward1478"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "UpsamplingBilinear2dBackward479"
    type: "Deconvolution"
    bottom: "AddBackward1478"
    top: "UpsamplingBilinear2dBackward479"
    convolution_param {
        num_output: 64
        group: 64
        kernel_size: 4
        stride: 2
        pad: 1
        weight_filler {
            type: "bilinear"
        }
        bias_term: false
    }
    param {
        lr_mult: 0
        decay_mult: 0
    }
}
layer {
    name: "PReLUBackward480"
    type: "PReLU"
    bottom: "UpsamplingBilinear2dBackward479"
    top: "PReLUBackward480"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.028341081
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward481"
    type: "Convolution"
    bottom: "PReLUBackward480"
    top: "ConvNdBackward481"
    convolution_param {
        num_output: 32
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "PReLUBackward482"
    type: "PReLU"
    bottom: "ConvNdBackward481"
    top: "PReLUBackward482"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.040263765
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward483"
    type: "Convolution"
    bottom: "PReLUBackward482"
    top: "ConvNdBackward483"
    convolution_param {
        num_output: 32
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "UpsamplingBilinear2dBackward485"
    type: "Deconvolution"
    bottom: "AddBackward1471"
    top: "UpsamplingBilinear2dBackward485"
    convolution_param {
        num_output: 64
        group: 64
        kernel_size: 4
        stride: 2
        pad: 1
        weight_filler {
            type: "bilinear"
        }
        bias_term: false
    }
    param {
        lr_mult: 0
        decay_mult: 0
    }
}
layer {
    name: "PReLUBackward486"
    type: "PReLU"
    bottom: "UpsamplingBilinear2dBackward485"
    top: "PReLUBackward486"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.27218503
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward487"
    type: "Convolution"
    bottom: "PReLUBackward486"
    top: "ConvNdBackward487"
    convolution_param {
        num_output: 32
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "PReLUBackward488"
    type: "PReLU"
    bottom: "ConvNdBackward487"
    top: "PReLUBackward488"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.05031759
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward489"
    type: "Convolution"
    bottom: "PReLUBackward488"
    top: "ConvNdBackward489"
    convolution_param {
        num_output: 32
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "UpsamplingBilinear2dBackward491"
    type: "Deconvolution"
    bottom: "AddBackward1464"
    top: "UpsamplingBilinear2dBackward491"
    convolution_param {
        num_output: 64
        group: 64
        kernel_size: 4
        stride: 2
        pad: 1
        weight_filler {
            type: "bilinear"
        }
        bias_term: false
    }
    param {
        lr_mult: 0
        decay_mult: 0
    }
}
layer {
    name: "PReLUBackward492"
    type: "PReLU"
    bottom: "UpsamplingBilinear2dBackward491"
    top: "PReLUBackward492"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.19640504
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward493"
    type: "Convolution"
    bottom: "PReLUBackward492"
    top: "ConvNdBackward493"
    convolution_param {
        num_output: 32
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "PReLUBackward494"
    type: "PReLU"
    bottom: "ConvNdBackward493"
    top: "PReLUBackward494"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.06302087
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward495"
    type: "Convolution"
    bottom: "PReLUBackward494"
    top: "ConvNdBackward495"
    convolution_param {
        num_output: 32
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "PReLUBackward497"
    type: "PReLU"
    bottom: "AddBackward1411"
    top: "PReLUBackward497"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.37131372
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward498"
    type: "Convolution"
    bottom: "PReLUBackward497"
    top: "ConvNdBackward498"
    convolution_param {
        num_output: 32
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "PReLUBackward499"
    type: "PReLU"
    bottom: "ConvNdBackward498"
    top: "PReLUBackward499"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.294545
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward500"
    type: "Convolution"
    bottom: "PReLUBackward499"
    top: "ConvNdBackward500"
    convolution_param {
        num_output: 32
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "AddBackward1502"
    type: "Eltwise"
    bottom: "ConvNdBackward500"
    bottom: "AddBackward1411"
    top: "AddBackward1502"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "AddBackward1503"
    type: "Eltwise"
    bottom: "ConvNdBackward495"
    bottom: "AddBackward1502"
    top: "AddBackward1503"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "PReLUBackward504"
    type: "PReLU"
    bottom: "AddBackward1503"
    top: "PReLUBackward504"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.36338666
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward505"
    type: "Convolution"
    bottom: "PReLUBackward504"
    top: "ConvNdBackward505"
    convolution_param {
        num_output: 32
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "PReLUBackward506"
    type: "PReLU"
    bottom: "ConvNdBackward505"
    top: "PReLUBackward506"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.007936802
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward507"
    type: "Convolution"
    bottom: "PReLUBackward506"
    top: "ConvNdBackward507"
    convolution_param {
        num_output: 32
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "AddBackward1509"
    type: "Eltwise"
    bottom: "ConvNdBackward507"
    bottom: "AddBackward1503"
    top: "AddBackward1509"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "AddBackward1510"
    type: "Eltwise"
    bottom: "ConvNdBackward489"
    bottom: "AddBackward1509"
    top: "AddBackward1510"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "PReLUBackward511"
    type: "PReLU"
    bottom: "AddBackward1510"
    top: "PReLUBackward511"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.39681333
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward512"
    type: "Convolution"
    bottom: "PReLUBackward511"
    top: "ConvNdBackward512"
    convolution_param {
        num_output: 32
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "PReLUBackward513"
    type: "PReLU"
    bottom: "ConvNdBackward512"
    top: "PReLUBackward513"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: -0.0072683436
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward514"
    type: "Convolution"
    bottom: "PReLUBackward513"
    top: "ConvNdBackward514"
    convolution_param {
        num_output: 32
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "AddBackward1516"
    type: "Eltwise"
    bottom: "ConvNdBackward514"
    bottom: "AddBackward1510"
    top: "AddBackward1516"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "AddBackward1517"
    type: "Eltwise"
    bottom: "ConvNdBackward483"
    bottom: "AddBackward1516"
    top: "AddBackward1517"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "PReLUBackward518"
    type: "PReLU"
    bottom: "AddBackward1517"
    top: "PReLUBackward518"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.34181008
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward519"
    type: "Convolution"
    bottom: "PReLUBackward518"
    top: "ConvNdBackward519"
    convolution_param {
        num_output: 3
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "PReLUBackward520"
    type: "PReLU"
    bottom: "ConvNdBackward519"
    top: "PReLUBackward520"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.4230891
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward521"
    type: "Convolution"
    bottom: "PReLUBackward520"
    top: "ConvNdBackward521"
    convolution_param {
        num_output: 3
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
