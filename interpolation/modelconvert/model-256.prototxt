name: "pytorch"
input: "data1"
input: "data2"
input_shape {
  dim: 1
  dim: 3
  dim: 256
  dim: 256
}
input_shape {
  dim: 1
  dim: 3
  dim: 256
  dim: 256
}
layer {
    name: "ConvNdBackward1"
    type: "Convolution"
    bottom: "data1"
    top: "ConvNdBackward1"
    convolution_param {
        num_output: 16
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 2
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward2"
    type: "ReLU"
    bottom: "ConvNdBackward1"
    top: "ConvNdBackward1"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward3"
    type: "Convolution"
    bottom: "ConvNdBackward1"
    top: "ConvNdBackward3"
    convolution_param {
        num_output: 16
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward4"
    type: "ReLU"
    bottom: "ConvNdBackward3"
    top: "ConvNdBackward3"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward5"
    type: "Convolution"
    bottom: "data2"
    top: "ConvNdBackward5"
    convolution_param {
        num_output: 16
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 2
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward6"
    type: "ReLU"
    bottom: "ConvNdBackward5"
    top: "ConvNdBackward5"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward7"
    type: "Convolution"
    bottom: "ConvNdBackward5"
    top: "ConvNdBackward7"
    convolution_param {
        num_output: 16
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward8"
    type: "ReLU"
    bottom: "ConvNdBackward7"
    top: "ConvNdBackward7"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward10"
    type: "Convolution"
    bottom: "ConvNdBackward3"
    top: "ConvNdBackward10"
    convolution_param {
        num_output: 32
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 2
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward11"
    type: "ReLU"
    bottom: "ConvNdBackward10"
    top: "ConvNdBackward10"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward12"
    type: "Convolution"
    bottom: "ConvNdBackward10"
    top: "ConvNdBackward12"
    convolution_param {
        num_output: 32
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward13"
    type: "ReLU"
    bottom: "ConvNdBackward12"
    top: "ConvNdBackward12"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward15"
    type: "Convolution"
    bottom: "ConvNdBackward7"
    top: "ConvNdBackward15"
    convolution_param {
        num_output: 32
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 2
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward16"
    type: "ReLU"
    bottom: "ConvNdBackward15"
    top: "ConvNdBackward15"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward17"
    type: "Convolution"
    bottom: "ConvNdBackward15"
    top: "ConvNdBackward17"
    convolution_param {
        num_output: 32
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward18"
    type: "ReLU"
    bottom: "ConvNdBackward17"
    top: "ConvNdBackward17"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward20"
    type: "Convolution"
    bottom: "ConvNdBackward12"
    top: "ConvNdBackward20"
    convolution_param {
        num_output: 64
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 2
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward21"
    type: "ReLU"
    bottom: "ConvNdBackward20"
    top: "ConvNdBackward20"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward22"
    type: "Convolution"
    bottom: "ConvNdBackward20"
    top: "ConvNdBackward22"
    convolution_param {
        num_output: 64
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward23"
    type: "ReLU"
    bottom: "ConvNdBackward22"
    top: "ConvNdBackward22"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward25"
    type: "Convolution"
    bottom: "ConvNdBackward17"
    top: "ConvNdBackward25"
    convolution_param {
        num_output: 64
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 2
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward26"
    type: "ReLU"
    bottom: "ConvNdBackward25"
    top: "ConvNdBackward25"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward27"
    type: "Convolution"
    bottom: "ConvNdBackward25"
    top: "ConvNdBackward27"
    convolution_param {
        num_output: 64
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward28"
    type: "ReLU"
    bottom: "ConvNdBackward27"
    top: "ConvNdBackward27"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward30"
    type: "Convolution"
    bottom: "ConvNdBackward22"
    top: "ConvNdBackward30"
    convolution_param {
        num_output: 96
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 2
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward31"
    type: "ReLU"
    bottom: "ConvNdBackward30"
    top: "ConvNdBackward30"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward32"
    type: "Convolution"
    bottom: "ConvNdBackward30"
    top: "ConvNdBackward32"
    convolution_param {
        num_output: 96
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward33"
    type: "ReLU"
    bottom: "ConvNdBackward32"
    top: "ConvNdBackward32"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward35"
    type: "Convolution"
    bottom: "ConvNdBackward27"
    top: "ConvNdBackward35"
    convolution_param {
        num_output: 96
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 2
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward36"
    type: "ReLU"
    bottom: "ConvNdBackward35"
    top: "ConvNdBackward35"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward37"
    type: "Convolution"
    bottom: "ConvNdBackward35"
    top: "ConvNdBackward37"
    convolution_param {
        num_output: 96
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward38"
    type: "ReLU"
    bottom: "ConvNdBackward37"
    top: "ConvNdBackward37"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward40"
    type: "Convolution"
    bottom: "ConvNdBackward32"
    top: "ConvNdBackward40"
    convolution_param {
        num_output: 128
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 2
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward41"
    type: "ReLU"
    bottom: "ConvNdBackward40"
    top: "ConvNdBackward40"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward42"
    type: "Convolution"
    bottom: "ConvNdBackward40"
    top: "ConvNdBackward42"
    convolution_param {
        num_output: 128
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward43"
    type: "ReLU"
    bottom: "ConvNdBackward42"
    top: "ConvNdBackward42"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward45"
    type: "Convolution"
    bottom: "ConvNdBackward37"
    top: "ConvNdBackward45"
    convolution_param {
        num_output: 128
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 2
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward46"
    type: "ReLU"
    bottom: "ConvNdBackward45"
    top: "ConvNdBackward45"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward47"
    type: "Convolution"
    bottom: "ConvNdBackward45"
    top: "ConvNdBackward47"
    convolution_param {
        num_output: 128
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward48"
    type: "ReLU"
    bottom: "ConvNdBackward47"
    top: "ConvNdBackward47"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward50"
    type: "Convolution"
    bottom: "ConvNdBackward42"
    top: "ConvNdBackward50"
    convolution_param {
        num_output: 192
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 2
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward51"
    type: "ReLU"
    bottom: "ConvNdBackward50"
    top: "ConvNdBackward50"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward52"
    type: "Convolution"
    bottom: "ConvNdBackward50"
    top: "ConvNdBackward52"
    convolution_param {
        num_output: 192
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward53"
    type: "ReLU"
    bottom: "ConvNdBackward52"
    top: "ConvNdBackward52"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward55"
    type: "Convolution"
    bottom: "ConvNdBackward47"
    top: "ConvNdBackward55"
    convolution_param {
        num_output: 192
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 2
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward56"
    type: "ReLU"
    bottom: "ConvNdBackward55"
    top: "ConvNdBackward55"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward57"
    type: "Convolution"
    bottom: "ConvNdBackward55"
    top: "ConvNdBackward57"
    convolution_param {
        num_output: 192
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward58"
    type: "ReLU"
    bottom: "ConvNdBackward57"
    top: "ConvNdBackward57"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "IndexBackward60"
    type: "Slice"
    bottom: "ConvNdBackward57"
    top: "IndexBackward60"
    top: "IndexBackward60_zcam2"
    slice_param {
        axis: 1
        slice_point: 2
    }
}
layer {
    name: "SubBackward162"
    type: "Eltwise"
    bottom: "IndexBackward60"
    bottom: "IndexBackward60"
    top: "SubBackward162"
    eltwise_param {
        operation: SUM
        coeff: 1
        coeff: -1
    }
}
layer {
    name: "Resample2dFunctionBackward63"
    type: "FlowWarp"
    bottom: "ConvNdBackward57"
    bottom: "SubBackward162"
    top: "Resample2dFunctionBackward63"
}
layer {
    name: "CatBackward65"
    type: "Concat"
    bottom: "ConvNdBackward52"
    bottom: "Resample2dFunctionBackward63"
    bottom: "SubBackward162"
    top: "CatBackward65"
    concat_param {
        axis: 1
    }
}
layer {
    name: "ConvNdBackward66"
    type: "Convolution"
    bottom: "CatBackward65"
    top: "ConvNdBackward66"
    convolution_param {
        num_output: 128
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward67"
    type: "ReLU"
    bottom: "ConvNdBackward66"
    top: "ConvNdBackward66"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward68"
    type: "Convolution"
    bottom: "ConvNdBackward66"
    top: "ConvNdBackward68"
    convolution_param {
        num_output: 128
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward69"
    type: "ReLU"
    bottom: "ConvNdBackward68"
    top: "ConvNdBackward68"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward70"
    type: "Convolution"
    bottom: "ConvNdBackward68"
    top: "ConvNdBackward70"
    convolution_param {
        num_output: 96
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward71"
    type: "ReLU"
    bottom: "ConvNdBackward70"
    top: "ConvNdBackward70"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward72"
    type: "Convolution"
    bottom: "ConvNdBackward70"
    top: "ConvNdBackward72"
    convolution_param {
        num_output: 64
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward73"
    type: "ReLU"
    bottom: "ConvNdBackward72"
    top: "ConvNdBackward72"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward74"
    type: "Convolution"
    bottom: "ConvNdBackward72"
    top: "ConvNdBackward74"
    convolution_param {
        num_output: 32
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward75"
    type: "ReLU"
    bottom: "ConvNdBackward74"
    top: "ConvNdBackward74"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward76"
    type: "Convolution"
    bottom: "ConvNdBackward74"
    top: "ConvNdBackward76"
    convolution_param {
        num_output: 2
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "UpsamplingBilinear2dBackward77"
    type: "Deconvolution"
    bottom: "ConvNdBackward76"
    top: "UpsamplingBilinear2dBackward77"
    convolution_param {
        num_output: 2
        group: 2
        kernel_size: 4
        stride: 2
        pad: 1
        weight_filler {
            type: "bilinear"
        }
        bias_term: false
    }
    param {
        lr_mult: 0
        decay_mult: 0
    }
}
layer {
    name: "Resample2dFunctionBackward78"
    type: "FlowWarp"
    bottom: "ConvNdBackward47"
    bottom: "UpsamplingBilinear2dBackward77"
    top: "Resample2dFunctionBackward78"
}
layer {
    name: "CatBackward80"
    type: "Concat"
    bottom: "ConvNdBackward42"
    bottom: "Resample2dFunctionBackward78"
    bottom: "UpsamplingBilinear2dBackward77"
    top: "CatBackward80"
    concat_param {
        axis: 1
    }
}
layer {
    name: "ConvNdBackward81"
    type: "Convolution"
    bottom: "CatBackward80"
    top: "ConvNdBackward81"
    convolution_param {
        num_output: 128
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward82"
    type: "ReLU"
    bottom: "ConvNdBackward81"
    top: "ConvNdBackward81"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward83"
    type: "Convolution"
    bottom: "ConvNdBackward81"
    top: "ConvNdBackward83"
    convolution_param {
        num_output: 128
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward84"
    type: "ReLU"
    bottom: "ConvNdBackward83"
    top: "ConvNdBackward83"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward85"
    type: "Convolution"
    bottom: "ConvNdBackward83"
    top: "ConvNdBackward85"
    convolution_param {
        num_output: 96
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward86"
    type: "ReLU"
    bottom: "ConvNdBackward85"
    top: "ConvNdBackward85"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward87"
    type: "Convolution"
    bottom: "ConvNdBackward85"
    top: "ConvNdBackward87"
    convolution_param {
        num_output: 64
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward88"
    type: "ReLU"
    bottom: "ConvNdBackward87"
    top: "ConvNdBackward87"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward89"
    type: "Convolution"
    bottom: "ConvNdBackward87"
    top: "ConvNdBackward89"
    convolution_param {
        num_output: 32
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward90"
    type: "ReLU"
    bottom: "ConvNdBackward89"
    top: "ConvNdBackward89"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward91"
    type: "Convolution"
    bottom: "ConvNdBackward89"
    top: "ConvNdBackward91"
    convolution_param {
        num_output: 2
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "UpsamplingBilinear2dBackward92"
    type: "Deconvolution"
    bottom: "ConvNdBackward91"
    top: "UpsamplingBilinear2dBackward92"
    convolution_param {
        num_output: 2
        group: 2
        kernel_size: 4
        stride: 2
        pad: 1
        weight_filler {
            type: "bilinear"
        }
        bias_term: false
    }
    param {
        lr_mult: 0
        decay_mult: 0
    }
}
layer {
    name: "Resample2dFunctionBackward93"
    type: "FlowWarp"
    bottom: "ConvNdBackward37"
    bottom: "UpsamplingBilinear2dBackward92"
    top: "Resample2dFunctionBackward93"
}
layer {
    name: "CatBackward95"
    type: "Concat"
    bottom: "ConvNdBackward32"
    bottom: "Resample2dFunctionBackward93"
    bottom: "UpsamplingBilinear2dBackward92"
    top: "CatBackward95"
    concat_param {
        axis: 1
    }
}
layer {
    name: "ConvNdBackward96"
    type: "Convolution"
    bottom: "CatBackward95"
    top: "ConvNdBackward96"
    convolution_param {
        num_output: 128
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward97"
    type: "ReLU"
    bottom: "ConvNdBackward96"
    top: "ConvNdBackward96"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward98"
    type: "Convolution"
    bottom: "ConvNdBackward96"
    top: "ConvNdBackward98"
    convolution_param {
        num_output: 128
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward99"
    type: "ReLU"
    bottom: "ConvNdBackward98"
    top: "ConvNdBackward98"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward100"
    type: "Convolution"
    bottom: "ConvNdBackward98"
    top: "ConvNdBackward100"
    convolution_param {
        num_output: 96
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward101"
    type: "ReLU"
    bottom: "ConvNdBackward100"
    top: "ConvNdBackward100"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward102"
    type: "Convolution"
    bottom: "ConvNdBackward100"
    top: "ConvNdBackward102"
    convolution_param {
        num_output: 64
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward103"
    type: "ReLU"
    bottom: "ConvNdBackward102"
    top: "ConvNdBackward102"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward104"
    type: "Convolution"
    bottom: "ConvNdBackward102"
    top: "ConvNdBackward104"
    convolution_param {
        num_output: 32
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward105"
    type: "ReLU"
    bottom: "ConvNdBackward104"
    top: "ConvNdBackward104"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward106"
    type: "Convolution"
    bottom: "ConvNdBackward104"
    top: "ConvNdBackward106"
    convolution_param {
        num_output: 2
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "UpsamplingBilinear2dBackward107"
    type: "Deconvolution"
    bottom: "ConvNdBackward106"
    top: "UpsamplingBilinear2dBackward107"
    convolution_param {
        num_output: 2
        group: 2
        kernel_size: 4
        stride: 2
        pad: 1
        weight_filler {
            type: "bilinear"
        }
        bias_term: false
    }
    param {
        lr_mult: 0
        decay_mult: 0
    }
}
layer {
    name: "Resample2dFunctionBackward108"
    type: "FlowWarp"
    bottom: "ConvNdBackward27"
    bottom: "UpsamplingBilinear2dBackward107"
    top: "Resample2dFunctionBackward108"
}
layer {
    name: "CatBackward110"
    type: "Concat"
    bottom: "ConvNdBackward22"
    bottom: "Resample2dFunctionBackward108"
    bottom: "UpsamplingBilinear2dBackward107"
    top: "CatBackward110"
    concat_param {
        axis: 1
    }
}
layer {
    name: "ConvNdBackward111"
    type: "Convolution"
    bottom: "CatBackward110"
    top: "ConvNdBackward111"
    convolution_param {
        num_output: 128
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward112"
    type: "ReLU"
    bottom: "ConvNdBackward111"
    top: "ConvNdBackward111"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward113"
    type: "Convolution"
    bottom: "ConvNdBackward111"
    top: "ConvNdBackward113"
    convolution_param {
        num_output: 128
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward114"
    type: "ReLU"
    bottom: "ConvNdBackward113"
    top: "ConvNdBackward113"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward115"
    type: "Convolution"
    bottom: "ConvNdBackward113"
    top: "ConvNdBackward115"
    convolution_param {
        num_output: 96
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward116"
    type: "ReLU"
    bottom: "ConvNdBackward115"
    top: "ConvNdBackward115"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward117"
    type: "Convolution"
    bottom: "ConvNdBackward115"
    top: "ConvNdBackward117"
    convolution_param {
        num_output: 64
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward118"
    type: "ReLU"
    bottom: "ConvNdBackward117"
    top: "ConvNdBackward117"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward119"
    type: "Convolution"
    bottom: "ConvNdBackward117"
    top: "ConvNdBackward119"
    convolution_param {
        num_output: 32
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward120"
    type: "ReLU"
    bottom: "ConvNdBackward119"
    top: "ConvNdBackward119"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward121"
    type: "Convolution"
    bottom: "ConvNdBackward119"
    top: "ConvNdBackward121"
    convolution_param {
        num_output: 2
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "UpsamplingBilinear2dBackward122"
    type: "Deconvolution"
    bottom: "ConvNdBackward121"
    top: "UpsamplingBilinear2dBackward122"
    convolution_param {
        num_output: 2
        group: 2
        kernel_size: 4
        stride: 2
        pad: 1
        weight_filler {
            type: "bilinear"
        }
        bias_term: false
    }
    param {
        lr_mult: 0
        decay_mult: 0
    }
}
layer {
    name: "Resample2dFunctionBackward123"
    type: "FlowWarp"
    bottom: "ConvNdBackward17"
    bottom: "UpsamplingBilinear2dBackward122"
    top: "Resample2dFunctionBackward123"
}
layer {
    name: "CatBackward125"
    type: "Concat"
    bottom: "ConvNdBackward12"
    bottom: "Resample2dFunctionBackward123"
    bottom: "UpsamplingBilinear2dBackward122"
    top: "CatBackward125"
    concat_param {
        axis: 1
    }
}
layer {
    name: "ConvNdBackward126"
    type: "Convolution"
    bottom: "CatBackward125"
    top: "ConvNdBackward126"
    convolution_param {
        num_output: 128
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward127"
    type: "ReLU"
    bottom: "ConvNdBackward126"
    top: "ConvNdBackward126"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward128"
    type: "Convolution"
    bottom: "ConvNdBackward126"
    top: "ConvNdBackward128"
    convolution_param {
        num_output: 128
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward129"
    type: "ReLU"
    bottom: "ConvNdBackward128"
    top: "ConvNdBackward128"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward130"
    type: "Convolution"
    bottom: "ConvNdBackward128"
    top: "ConvNdBackward130"
    convolution_param {
        num_output: 96
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward131"
    type: "ReLU"
    bottom: "ConvNdBackward130"
    top: "ConvNdBackward130"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward132"
    type: "Convolution"
    bottom: "ConvNdBackward130"
    top: "ConvNdBackward132"
    convolution_param {
        num_output: 64
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward133"
    type: "ReLU"
    bottom: "ConvNdBackward132"
    top: "ConvNdBackward132"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward134"
    type: "Convolution"
    bottom: "ConvNdBackward132"
    top: "ConvNdBackward134"
    convolution_param {
        num_output: 32
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward135"
    type: "ReLU"
    bottom: "ConvNdBackward134"
    top: "ConvNdBackward134"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward136"
    type: "Convolution"
    bottom: "ConvNdBackward134"
    top: "ConvNdBackward136"
    convolution_param {
        num_output: 2
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "UpsamplingBilinear2dBackward137"
    type: "Deconvolution"
    bottom: "ConvNdBackward136"
    top: "UpsamplingBilinear2dBackward137"
    convolution_param {
        num_output: 2
        group: 2
        kernel_size: 4
        stride: 2
        pad: 1
        weight_filler {
            type: "bilinear"
        }
        bias_term: false
    }
    param {
        lr_mult: 0
        decay_mult: 0
    }
}
layer {
    name: "Resample2dFunctionBackward138"
    type: "FlowWarp"
    bottom: "ConvNdBackward7"
    bottom: "UpsamplingBilinear2dBackward137"
    top: "Resample2dFunctionBackward138"
}
layer {
    name: "CatBackward140"
    type: "Concat"
    bottom: "ConvNdBackward3"
    bottom: "Resample2dFunctionBackward138"
    bottom: "UpsamplingBilinear2dBackward137"
    top: "CatBackward140"
    concat_param {
        axis: 1
    }
}
layer {
    name: "ConvNdBackward141"
    type: "Convolution"
    bottom: "CatBackward140"
    top: "ConvNdBackward141"
    convolution_param {
        num_output: 128
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward142"
    type: "ReLU"
    bottom: "ConvNdBackward141"
    top: "ConvNdBackward141"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward143"
    type: "Convolution"
    bottom: "ConvNdBackward141"
    top: "ConvNdBackward143"
    convolution_param {
        num_output: 128
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward144"
    type: "ReLU"
    bottom: "ConvNdBackward143"
    top: "ConvNdBackward143"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward145"
    type: "Convolution"
    bottom: "ConvNdBackward143"
    top: "ConvNdBackward145"
    convolution_param {
        num_output: 96
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward146"
    type: "ReLU"
    bottom: "ConvNdBackward145"
    top: "ConvNdBackward145"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward147"
    type: "Convolution"
    bottom: "ConvNdBackward145"
    top: "ConvNdBackward147"
    convolution_param {
        num_output: 64
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward148"
    type: "ReLU"
    bottom: "ConvNdBackward147"
    top: "ConvNdBackward147"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward149"
    type: "Convolution"
    bottom: "ConvNdBackward147"
    top: "ConvNdBackward149"
    convolution_param {
        num_output: 32
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "LeakyReluBackward150"
    type: "ReLU"
    bottom: "ConvNdBackward149"
    top: "ConvNdBackward149"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward152"
    type: "Convolution"
    bottom: "ConvNdBackward149"
    top: "ConvNdBackward152"
    convolution_param {
        num_output: 2
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "CatBackward153"
    type: "Concat"
    bottom: "ConvNdBackward149"
    bottom: "ConvNdBackward152"
    top: "CatBackward153"
    concat_param {
        axis: 1
    }
}
layer {
    name: "ConvNdBackward154"
    type: "Convolution"
    bottom: "CatBackward153"
    top: "ConvNdBackward154"
    convolution_param {
        num_output: 128
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: false
    }
}
layer {
    name: "LeakyReluBackward155"
    type: "ReLU"
    bottom: "ConvNdBackward154"
    top: "ConvNdBackward154"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward156"
    type: "Convolution"
    bottom: "ConvNdBackward154"
    top: "ConvNdBackward156"
    convolution_param {
        num_output: 128
        pad_h: 2
        pad_w: 2
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 2
        bias_term: false
    }
}
layer {
    name: "LeakyReluBackward157"
    type: "ReLU"
    bottom: "ConvNdBackward156"
    top: "ConvNdBackward156"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward158"
    type: "Convolution"
    bottom: "ConvNdBackward156"
    top: "ConvNdBackward158"
    convolution_param {
        num_output: 128
        pad_h: 4
        pad_w: 4
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 4
        bias_term: false
    }
}
layer {
    name: "LeakyReluBackward159"
    type: "ReLU"
    bottom: "ConvNdBackward158"
    top: "ConvNdBackward158"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward160"
    type: "Convolution"
    bottom: "ConvNdBackward158"
    top: "ConvNdBackward160"
    convolution_param {
        num_output: 96
        pad_h: 8
        pad_w: 8
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 8
        bias_term: false
    }
}
layer {
    name: "LeakyReluBackward161"
    type: "ReLU"
    bottom: "ConvNdBackward160"
    top: "ConvNdBackward160"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward162"
    type: "Convolution"
    bottom: "ConvNdBackward160"
    top: "ConvNdBackward162"
    convolution_param {
        num_output: 64
        pad_h: 16
        pad_w: 16
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 16
        bias_term: false
    }
}
layer {
    name: "LeakyReluBackward163"
    type: "ReLU"
    bottom: "ConvNdBackward162"
    top: "ConvNdBackward162"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward164"
    type: "Convolution"
    bottom: "ConvNdBackward162"
    top: "ConvNdBackward164"
    convolution_param {
        num_output: 32
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: false
    }
}
layer {
    name: "LeakyReluBackward165"
    type: "ReLU"
    bottom: "ConvNdBackward164"
    top: "ConvNdBackward164"
    relu_param {
        negative_slope: 0.01
    }
}
layer {
    name: "ConvNdBackward166"
    type: "Convolution"
    bottom: "ConvNdBackward164"
    top: "ConvNdBackward166"
    convolution_param {
        num_output: 2
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: false
    }
}
layer {
    name: "AddBackward1168"
    type: "Eltwise"
    bottom: "ConvNdBackward166"
    bottom: "ConvNdBackward152"
    top: "AddBackward1168"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "UpsamplingBilinear2dBackward169"
    type: "Deconvolution"
    bottom: "AddBackward1168"
    top: "UpsamplingBilinear2dBackward169"
    convolution_param {
        num_output: 2
        group: 2
        kernel_size: 4
        stride: 2
        pad: 1
        weight_filler {
            type: "bilinear"
        }
        bias_term: false
    }
    param {
        lr_mult: 0
        decay_mult: 0
    }
}
layer {
    name: "MulBackward0170"
    type: "Eltwise"
    bottom: "UpsamplingBilinear2dBackward169"
    top: "MulBackward0170"
    eltwise_param {
        operation: MUL
        mulvalue: 0.5
    }
}
layer {
    name: "Resample2dFunctionBackward171"
    type: "FlowWarp"
    bottom: "data1"
    bottom: "MulBackward0170"
    top: "Resample2dFunctionBackward171"
}
layer {
    name: "ConvNdBackward172"
    type: "Convolution"
    bottom: "data1"
    top: "ConvNdBackward172"
    convolution_param {
        num_output: 64
        pad_h: 3
        pad_w: 3
        kernel_h: 7
        kernel_w: 7
        stride: 1
        dilation: 1
        bias_term: false
    }
}
layer {
    name: "Resample2dFunctionBackward174"
    type: "FlowWarp"
    bottom: "ConvNdBackward172"
    bottom: "MulBackward0170"
    top: "Resample2dFunctionBackward174"
}
layer {
    name: "MulBackward0176"
    type: "Eltwise"
    bottom: "MulBackward0170"
    top: "MulBackward0176"
    eltwise_param {
        operation: MUL
        mulvalue: 1.0
    }
}
layer {
    name: "Resample2dFunctionBackward177"
    type: "FlowWarp"
    bottom: "data2"
    bottom: "MulBackward0176"
    top: "Resample2dFunctionBackward177"
}
layer {
    name: "ConvNdBackward178"
    type: "Convolution"
    bottom: "data2"
    top: "ConvNdBackward178"
    convolution_param {
        num_output: 64
        pad_h: 3
        pad_w: 3
        kernel_h: 7
        kernel_w: 7
        stride: 1
        dilation: 1
        bias_term: false
    }
}
layer {
    name: "Resample2dFunctionBackward180"
    type: "FlowWarp"
    bottom: "ConvNdBackward178"
    bottom: "MulBackward0176"
    top: "Resample2dFunctionBackward180"
}
layer {
    name: "CatBackward181"
    type: "Concat"
    bottom: "Resample2dFunctionBackward171"
    bottom: "Resample2dFunctionBackward174"
    bottom: "Resample2dFunctionBackward177"
    bottom: "Resample2dFunctionBackward180"
    top: "CatBackward181"
    concat_param {
        axis: 1
    }
}
layer {
    name: "PReLUBackward182"
    type: "PReLU"
    bottom: "CatBackward181"
    top: "CatBackward181"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.6964133
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward183"
    type: "Convolution"
    bottom: "CatBackward181"
    top: "ConvNdBackward183"
    convolution_param {
        num_output: 32
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "PReLUBackward184"
    type: "PReLU"
    bottom: "ConvNdBackward183"
    top: "ConvNdBackward183"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.3497803
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward185"
    type: "Convolution"
    bottom: "ConvNdBackward183"
    top: "ConvNdBackward185"
    convolution_param {
        num_output: 32
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "PReLUBackward186"
    type: "PReLU"
    bottom: "ConvNdBackward185"
    top: "PReLUBackward186"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.26257417
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward187"
    type: "Convolution"
    bottom: "PReLUBackward186"
    top: "ConvNdBackward187"
    convolution_param {
        num_output: 64
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 2
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "PReLUBackward188"
    type: "PReLU"
    bottom: "ConvNdBackward187"
    top: "ConvNdBackward187"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.10766693
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward189"
    type: "Convolution"
    bottom: "ConvNdBackward187"
    top: "ConvNdBackward189"
    convolution_param {
        num_output: 64
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "PReLUBackward190"
    type: "PReLU"
    bottom: "ConvNdBackward189"
    top: "PReLUBackward190"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.0496447
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward191"
    type: "Convolution"
    bottom: "PReLUBackward190"
    top: "ConvNdBackward191"
    convolution_param {
        num_output: 96
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 2
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "PReLUBackward192"
    type: "PReLU"
    bottom: "ConvNdBackward191"
    top: "ConvNdBackward191"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.012810936
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward193"
    type: "Convolution"
    bottom: "ConvNdBackward191"
    top: "ConvNdBackward193"
    convolution_param {
        num_output: 96
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "PReLUBackward194"
    type: "PReLU"
    bottom: "ConvNdBackward193"
    top: "PReLUBackward194"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.018466815
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward195"
    type: "Convolution"
    bottom: "PReLUBackward194"
    top: "ConvNdBackward195"
    convolution_param {
        num_output: 96
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "PReLUBackward196"
    type: "PReLU"
    bottom: "ConvNdBackward195"
    top: "ConvNdBackward195"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.008857145
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward197"
    type: "Convolution"
    bottom: "ConvNdBackward195"
    top: "ConvNdBackward197"
    convolution_param {
        num_output: 96
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "AddBackward1199"
    type: "Eltwise"
    bottom: "ConvNdBackward197"
    bottom: "ConvNdBackward193"
    top: "AddBackward1199"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "PReLUBackward201"
    type: "PReLU"
    bottom: "ConvNdBackward189"
    top: "PReLUBackward201"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.076497845
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward202"
    type: "Convolution"
    bottom: "PReLUBackward201"
    top: "ConvNdBackward202"
    convolution_param {
        num_output: 64
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "PReLUBackward203"
    type: "PReLU"
    bottom: "ConvNdBackward202"
    top: "ConvNdBackward202"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: -0.004788584
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward204"
    type: "Convolution"
    bottom: "ConvNdBackward202"
    top: "ConvNdBackward204"
    convolution_param {
        num_output: 64
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "AddBackward1206"
    type: "Eltwise"
    bottom: "ConvNdBackward204"
    bottom: "ConvNdBackward189"
    top: "AddBackward1206"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "PReLUBackward208"
    type: "PReLU"
    bottom: "ConvNdBackward185"
    top: "PReLUBackward208"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.121568955
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward209"
    type: "Convolution"
    bottom: "PReLUBackward208"
    top: "ConvNdBackward209"
    convolution_param {
        num_output: 32
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "PReLUBackward210"
    type: "PReLU"
    bottom: "ConvNdBackward209"
    top: "ConvNdBackward209"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.03923847
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward211"
    type: "Convolution"
    bottom: "ConvNdBackward209"
    top: "ConvNdBackward211"
    convolution_param {
        num_output: 32
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "AddBackward1213"
    type: "Eltwise"
    bottom: "ConvNdBackward211"
    bottom: "ConvNdBackward185"
    top: "AddBackward1213"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "PReLUBackward214"
    type: "PReLU"
    bottom: "AddBackward1213"
    top: "PReLUBackward214"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.04673716
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward215"
    type: "Convolution"
    bottom: "PReLUBackward214"
    top: "ConvNdBackward215"
    convolution_param {
        num_output: 64
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 2
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "PReLUBackward216"
    type: "PReLU"
    bottom: "ConvNdBackward215"
    top: "ConvNdBackward215"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.031087188
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward217"
    type: "Convolution"
    bottom: "ConvNdBackward215"
    top: "ConvNdBackward217"
    convolution_param {
        num_output: 64
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "AddBackward1218"
    type: "Eltwise"
    bottom: "AddBackward1206"
    bottom: "ConvNdBackward217"
    top: "AddBackward1218"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "PReLUBackward219"
    type: "PReLU"
    bottom: "AddBackward1218"
    top: "PReLUBackward219"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.07670419
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward220"
    type: "Convolution"
    bottom: "PReLUBackward219"
    top: "ConvNdBackward220"
    convolution_param {
        num_output: 96
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 2
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "PReLUBackward221"
    type: "PReLU"
    bottom: "ConvNdBackward220"
    top: "ConvNdBackward220"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.035275783
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward222"
    type: "Convolution"
    bottom: "ConvNdBackward220"
    top: "ConvNdBackward222"
    convolution_param {
        num_output: 96
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "AddBackward1223"
    type: "Eltwise"
    bottom: "AddBackward1199"
    bottom: "ConvNdBackward222"
    top: "AddBackward1223"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "PReLUBackward224"
    type: "PReLU"
    bottom: "AddBackward1223"
    top: "PReLUBackward224"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.06381086
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward225"
    type: "Convolution"
    bottom: "PReLUBackward224"
    top: "ConvNdBackward225"
    convolution_param {
        num_output: 96
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "PReLUBackward226"
    type: "PReLU"
    bottom: "ConvNdBackward225"
    top: "ConvNdBackward225"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.017341536
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward227"
    type: "Convolution"
    bottom: "ConvNdBackward225"
    top: "ConvNdBackward227"
    convolution_param {
        num_output: 96
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "AddBackward1229"
    type: "Eltwise"
    bottom: "ConvNdBackward227"
    bottom: "AddBackward1223"
    top: "AddBackward1229"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "PReLUBackward230"
    type: "PReLU"
    bottom: "AddBackward1229"
    top: "PReLUBackward230"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.10087567
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward231"
    type: "Convolution"
    bottom: "PReLUBackward230"
    top: "ConvNdBackward231"
    convolution_param {
        num_output: 96
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "PReLUBackward232"
    type: "PReLU"
    bottom: "ConvNdBackward231"
    top: "ConvNdBackward231"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.029307222
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward233"
    type: "Convolution"
    bottom: "ConvNdBackward231"
    top: "ConvNdBackward233"
    convolution_param {
        num_output: 96
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "AddBackward1235"
    type: "Eltwise"
    bottom: "ConvNdBackward233"
    bottom: "AddBackward1229"
    top: "AddBackward1235"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "UpsamplingBilinear2dBackward236"
    type: "Deconvolution"
    bottom: "AddBackward1235"
    top: "UpsamplingBilinear2dBackward236"
    convolution_param {
        num_output: 96
        group: 96
        kernel_size: 4
        stride: 2
        pad: 1
        weight_filler {
            type: "bilinear"
        }
        bias_term: false
    }
    param {
        lr_mult: 0
        decay_mult: 0
    }
}
layer {
    name: "PReLUBackward237"
    type: "PReLU"
    bottom: "UpsamplingBilinear2dBackward236"
    top: "UpsamplingBilinear2dBackward236"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.03616495
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward238"
    type: "Convolution"
    bottom: "UpsamplingBilinear2dBackward236"
    top: "ConvNdBackward238"
    convolution_param {
        num_output: 64
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "PReLUBackward239"
    type: "PReLU"
    bottom: "ConvNdBackward238"
    top: "ConvNdBackward238"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.034601517
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward240"
    type: "Convolution"
    bottom: "ConvNdBackward238"
    top: "ConvNdBackward240"
    convolution_param {
        num_output: 64
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "UpsamplingBilinear2dBackward242"
    type: "Deconvolution"
    bottom: "AddBackward1229"
    top: "UpsamplingBilinear2dBackward242"
    convolution_param {
        num_output: 96
        group: 96
        kernel_size: 4
        stride: 2
        pad: 1
        weight_filler {
            type: "bilinear"
        }
        bias_term: false
    }
    param {
        lr_mult: 0
        decay_mult: 0
    }
}
layer {
    name: "PReLUBackward243"
    type: "PReLU"
    bottom: "UpsamplingBilinear2dBackward242"
    top: "UpsamplingBilinear2dBackward242"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.07581461
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward244"
    type: "Convolution"
    bottom: "UpsamplingBilinear2dBackward242"
    top: "ConvNdBackward244"
    convolution_param {
        num_output: 64
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "PReLUBackward245"
    type: "PReLU"
    bottom: "ConvNdBackward244"
    top: "ConvNdBackward244"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.032207888
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward246"
    type: "Convolution"
    bottom: "ConvNdBackward244"
    top: "ConvNdBackward246"
    convolution_param {
        num_output: 64
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "PReLUBackward248"
    type: "PReLU"
    bottom: "AddBackward1218"
    top: "PReLUBackward248"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.16769858
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward249"
    type: "Convolution"
    bottom: "PReLUBackward248"
    top: "ConvNdBackward249"
    convolution_param {
        num_output: 64
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "PReLUBackward250"
    type: "PReLU"
    bottom: "ConvNdBackward249"
    top: "ConvNdBackward249"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.054677952
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward251"
    type: "Convolution"
    bottom: "ConvNdBackward249"
    top: "ConvNdBackward251"
    convolution_param {
        num_output: 64
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "AddBackward1253"
    type: "Eltwise"
    bottom: "ConvNdBackward251"
    bottom: "AddBackward1218"
    top: "AddBackward1253"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "AddBackward1254"
    type: "Eltwise"
    bottom: "ConvNdBackward246"
    bottom: "AddBackward1253"
    top: "AddBackward1254"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "PReLUBackward255"
    type: "PReLU"
    bottom: "AddBackward1254"
    top: "PReLUBackward255"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.14653109
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward256"
    type: "Convolution"
    bottom: "PReLUBackward255"
    top: "ConvNdBackward256"
    convolution_param {
        num_output: 64
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "PReLUBackward257"
    type: "PReLU"
    bottom: "ConvNdBackward256"
    top: "ConvNdBackward256"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: -0.011456402
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward258"
    type: "Convolution"
    bottom: "ConvNdBackward256"
    top: "ConvNdBackward258"
    convolution_param {
        num_output: 64
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "AddBackward1260"
    type: "Eltwise"
    bottom: "ConvNdBackward258"
    bottom: "AddBackward1254"
    top: "AddBackward1260"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "AddBackward1261"
    type: "Eltwise"
    bottom: "ConvNdBackward240"
    bottom: "AddBackward1260"
    top: "AddBackward1261"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "UpsamplingBilinear2dBackward262"
    type: "Deconvolution"
    bottom: "AddBackward1261"
    top: "UpsamplingBilinear2dBackward262"
    convolution_param {
        num_output: 64
        group: 64
        kernel_size: 4
        stride: 2
        pad: 1
        weight_filler {
            type: "bilinear"
        }
        bias_term: false
    }
    param {
        lr_mult: 0
        decay_mult: 0
    }
}
layer {
    name: "PReLUBackward263"
    type: "PReLU"
    bottom: "UpsamplingBilinear2dBackward262"
    top: "UpsamplingBilinear2dBackward262"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.04803854
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward264"
    type: "Convolution"
    bottom: "UpsamplingBilinear2dBackward262"
    top: "ConvNdBackward264"
    convolution_param {
        num_output: 32
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "PReLUBackward265"
    type: "PReLU"
    bottom: "ConvNdBackward264"
    top: "ConvNdBackward264"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.03902043
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward266"
    type: "Convolution"
    bottom: "ConvNdBackward264"
    top: "ConvNdBackward266"
    convolution_param {
        num_output: 32
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "UpsamplingBilinear2dBackward268"
    type: "Deconvolution"
    bottom: "AddBackward1254"
    top: "UpsamplingBilinear2dBackward268"
    convolution_param {
        num_output: 64
        group: 64
        kernel_size: 4
        stride: 2
        pad: 1
        weight_filler {
            type: "bilinear"
        }
        bias_term: false
    }
    param {
        lr_mult: 0
        decay_mult: 0
    }
}
layer {
    name: "PReLUBackward269"
    type: "PReLU"
    bottom: "UpsamplingBilinear2dBackward268"
    top: "UpsamplingBilinear2dBackward268"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.11945911
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward270"
    type: "Convolution"
    bottom: "UpsamplingBilinear2dBackward268"
    top: "ConvNdBackward270"
    convolution_param {
        num_output: 32
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "PReLUBackward271"
    type: "PReLU"
    bottom: "ConvNdBackward270"
    top: "ConvNdBackward270"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.05801216
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward272"
    type: "Convolution"
    bottom: "ConvNdBackward270"
    top: "ConvNdBackward272"
    convolution_param {
        num_output: 32
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "PReLUBackward274"
    type: "PReLU"
    bottom: "AddBackward1213"
    top: "PReLUBackward274"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.06695192
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward275"
    type: "Convolution"
    bottom: "PReLUBackward274"
    top: "ConvNdBackward275"
    convolution_param {
        num_output: 32
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "PReLUBackward276"
    type: "PReLU"
    bottom: "ConvNdBackward275"
    top: "ConvNdBackward275"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.05351272
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward277"
    type: "Convolution"
    bottom: "ConvNdBackward275"
    top: "ConvNdBackward277"
    convolution_param {
        num_output: 32
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "AddBackward1279"
    type: "Eltwise"
    bottom: "ConvNdBackward277"
    bottom: "AddBackward1213"
    top: "AddBackward1279"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "AddBackward1280"
    type: "Eltwise"
    bottom: "ConvNdBackward272"
    bottom: "AddBackward1279"
    top: "AddBackward1280"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "PReLUBackward281"
    type: "PReLU"
    bottom: "AddBackward1280"
    top: "PReLUBackward281"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.097891346
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward282"
    type: "Convolution"
    bottom: "PReLUBackward281"
    top: "ConvNdBackward282"
    convolution_param {
        num_output: 32
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "PReLUBackward283"
    type: "PReLU"
    bottom: "ConvNdBackward282"
    top: "ConvNdBackward282"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.0150097795
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward284"
    type: "Convolution"
    bottom: "ConvNdBackward282"
    top: "ConvNdBackward284"
    convolution_param {
        num_output: 32
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "AddBackward1286"
    type: "Eltwise"
    bottom: "ConvNdBackward284"
    bottom: "AddBackward1280"
    top: "AddBackward1286"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "AddBackward1287"
    type: "Eltwise"
    bottom: "ConvNdBackward266"
    bottom: "AddBackward1286"
    top: "AddBackward1287"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "PReLUBackward288"
    type: "PReLU"
    bottom: "AddBackward1287"
    top: "AddBackward1287"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.28084838
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward289"
    type: "Convolution"
    bottom: "AddBackward1287"
    top: "ConvNdBackward289"
    convolution_param {
        num_output: 3
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
layer {
    name: "PReLUBackward290"
    type: "PReLU"
    bottom: "ConvNdBackward289"
    top: "ConvNdBackward289"
    param {
        lr_mult: 1
        decay_mult: 0
    }
    prelu_param {
        filler {
            value: 0.6564645
        }
        channel_shared: false
    }
}
layer {
    name: "ConvNdBackward291"
    type: "Convolution"
    bottom: "ConvNdBackward289"
    top: "ConvNdBackward291"
    convolution_param {
        num_output: 3
        pad_h: 1
        pad_w: 1
        kernel_h: 3
        kernel_w: 3
        stride: 1
        dilation: 1
        bias_term: true
    }
}
